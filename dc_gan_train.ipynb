{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAAAAA\n",
      "1.7774043083190918\n",
      "[0/25][0/16] Loss_D: 1.7774 Loss_G: 6.2913\n",
      "0.9682199954986572\n",
      "[0/25][1/16] Loss_D: 0.9682 Loss_G: 5.1473\n",
      "0.9515328407287598\n",
      "[0/25][2/16] Loss_D: 0.9515 Loss_G: 5.9763\n",
      "0.7880936861038208\n",
      "[0/25][3/16] Loss_D: 0.7881 Loss_G: 6.5571\n",
      "0.8632767796516418\n",
      "[0/25][4/16] Loss_D: 0.8633 Loss_G: 6.4296\n",
      "0.7717856168746948\n",
      "[0/25][5/16] Loss_D: 0.7718 Loss_G: 7.3537\n",
      "1.132309913635254\n",
      "[0/25][6/16] Loss_D: 1.1323 Loss_G: 6.8379\n",
      "1.0952506065368652\n",
      "[0/25][7/16] Loss_D: 1.0953 Loss_G: 9.4692\n",
      "0.7333135604858398\n",
      "[0/25][8/16] Loss_D: 0.7333 Loss_G: 7.8550\n",
      "0.827343225479126\n",
      "[0/25][9/16] Loss_D: 0.8273 Loss_G: 9.5525\n",
      "0.6661596894264221\n",
      "[0/25][10/16] Loss_D: 0.6662 Loss_G: 8.6829\n",
      "0.9478237628936768\n",
      "[0/25][11/16] Loss_D: 0.9478 Loss_G: 11.1878\n",
      "0.6579517126083374\n",
      "[0/25][12/16] Loss_D: 0.6580 Loss_G: 7.6384\n",
      "1.7174357175827026\n",
      "[0/25][13/16] Loss_D: 1.7174 Loss_G: 13.3064\n",
      "0.5582208037376404\n",
      "[0/25][14/16] Loss_D: 0.5582 Loss_G: 9.8988\n",
      "1.1880247592926025\n",
      "[0/25][15/16] Loss_D: 1.1880 Loss_G: 12.1123\n",
      "0.38049548864364624\n",
      "[1/25][0/16] Loss_D: 0.3805 Loss_G: 9.8086\n",
      "0.656171977519989\n",
      "[1/25][1/16] Loss_D: 0.6562 Loss_G: 12.0306\n",
      "0.3834453523159027\n",
      "[1/25][2/16] Loss_D: 0.3834 Loss_G: 9.4095\n",
      "1.0377453565597534\n",
      "[1/25][3/16] Loss_D: 1.0377 Loss_G: 15.1489\n",
      "0.316449373960495\n",
      "[1/25][4/16] Loss_D: 0.3164 Loss_G: 11.7780\n",
      "0.38840144872665405\n",
      "[1/25][5/16] Loss_D: 0.3884 Loss_G: 9.5720\n",
      "1.2851262092590332\n",
      "[1/25][6/16] Loss_D: 1.2851 Loss_G: 18.3537\n",
      "0.6453325152397156\n",
      "[1/25][7/16] Loss_D: 0.6453 Loss_G: 16.4281\n",
      "0.26918545365333557\n",
      "[1/25][8/16] Loss_D: 0.2692 Loss_G: 8.2564\n",
      "3.173030376434326\n",
      "[1/25][9/16] Loss_D: 3.1730 Loss_G: 17.9699\n",
      "0.5393124222755432\n",
      "[1/25][10/16] Loss_D: 0.5393 Loss_G: 18.1877\n",
      "0.25232577323913574\n",
      "[1/25][11/16] Loss_D: 0.2523 Loss_G: 12.9886\n",
      "0.2718053460121155\n",
      "[1/25][12/16] Loss_D: 0.2718 Loss_G: 6.2559\n",
      "3.535428524017334\n",
      "[1/25][13/16] Loss_D: 3.5354 Loss_G: 18.6864\n",
      "0.47904592752456665\n",
      "[1/25][14/16] Loss_D: 0.4790 Loss_G: 19.7537\n",
      "0.3132578730583191\n",
      "[1/25][15/16] Loss_D: 0.3133 Loss_G: 14.3802\n",
      "0.16395141184329987\n",
      "[2/25][0/16] Loss_D: 0.1640 Loss_G: 5.6338\n",
      "4.612685680389404\n",
      "[2/25][1/16] Loss_D: 4.6127 Loss_G: 18.6903\n",
      "0.7091678380966187\n",
      "[2/25][2/16] Loss_D: 0.7092 Loss_G: 20.5372\n",
      "0.3677395284175873\n",
      "[2/25][3/16] Loss_D: 0.3677 Loss_G: 16.8538\n",
      "0.14514105021953583\n",
      "[2/25][4/16] Loss_D: 0.1451 Loss_G: 8.9979\n",
      "0.5837446451187134\n",
      "[2/25][5/16] Loss_D: 0.5837 Loss_G: 11.7258\n",
      "0.15409517288208008\n",
      "[2/25][6/16] Loss_D: 0.1541 Loss_G: 9.9045\n",
      "0.25217458605766296\n",
      "[2/25][7/16] Loss_D: 0.2522 Loss_G: 8.4489\n",
      "0.5602501630783081\n",
      "[2/25][8/16] Loss_D: 0.5603 Loss_G: 14.1733\n",
      "0.27826935052871704\n",
      "[2/25][9/16] Loss_D: 0.2783 Loss_G: 13.2406\n",
      "0.4378169775009155\n",
      "[2/25][10/16] Loss_D: 0.4378 Loss_G: 7.6122\n",
      "0.8408833146095276\n",
      "[2/25][11/16] Loss_D: 0.8409 Loss_G: 16.1481\n",
      "0.22222380340099335\n",
      "[2/25][12/16] Loss_D: 0.2222 Loss_G: 16.6925\n",
      "0.1068912222981453\n",
      "[2/25][13/16] Loss_D: 0.1069 Loss_G: 12.5787\n",
      "0.0942002385854721\n",
      "[2/25][14/16] Loss_D: 0.0942 Loss_G: 6.2248\n",
      "1.6056034564971924\n",
      "[2/25][15/16] Loss_D: 1.6056 Loss_G: 21.1824\n",
      "0.8313948512077332\n",
      "[3/25][0/16] Loss_D: 0.8314 Loss_G: 22.8239\n",
      "0.32038164138793945\n",
      "[3/25][1/16] Loss_D: 0.3204 Loss_G: 19.9315\n",
      "0.1111937165260315\n",
      "[3/25][2/16] Loss_D: 0.1112 Loss_G: 13.6715\n",
      "0.04383554682135582\n",
      "[3/25][3/16] Loss_D: 0.0438 Loss_G: 6.2613\n",
      "1.8432998657226562\n",
      "[3/25][4/16] Loss_D: 1.8433 Loss_G: 20.2508\n",
      "0.26781001687049866\n",
      "[3/25][5/16] Loss_D: 0.2678 Loss_G: 23.6171\n",
      "0.800818681716919\n",
      "[3/25][6/16] Loss_D: 0.8008 Loss_G: 19.9934\n",
      "0.33076566457748413\n",
      "[3/25][7/16] Loss_D: 0.3308 Loss_G: 12.5811\n",
      "0.11518548429012299\n",
      "[3/25][8/16] Loss_D: 0.1152 Loss_G: 4.7211\n",
      "2.7463366985321045\n",
      "[3/25][9/16] Loss_D: 2.7463 Loss_G: 21.4045\n",
      "0.5341259837150574\n",
      "[3/25][10/16] Loss_D: 0.5341 Loss_G: 24.1147\n",
      "0.7274123430252075\n",
      "[3/25][11/16] Loss_D: 0.7274 Loss_G: 21.4694\n",
      "0.512622058391571\n",
      "[3/25][12/16] Loss_D: 0.5126 Loss_G: 14.6193\n",
      "0.5245123505592346\n",
      "[3/25][13/16] Loss_D: 0.5245 Loss_G: 6.0519\n",
      "1.4657535552978516\n",
      "[3/25][14/16] Loss_D: 1.4658 Loss_G: 18.8591\n",
      "0.1162555143237114\n",
      "[3/25][15/16] Loss_D: 0.1163 Loss_G: 21.9889\n",
      "0.2153087705373764\n",
      "[4/25][0/16] Loss_D: 0.2153 Loss_G: 20.3341\n",
      "0.45692434906959534\n",
      "[4/25][1/16] Loss_D: 0.4569 Loss_G: 15.3099\n",
      "0.20774394273757935\n",
      "[4/25][2/16] Loss_D: 0.2077 Loss_G: 8.1708\n",
      "0.49555110931396484\n",
      "[4/25][3/16] Loss_D: 0.4956 Loss_G: 11.0469\n",
      "0.02486993931233883\n",
      "[4/25][4/16] Loss_D: 0.0249 Loss_G: 10.1732\n",
      "0.16785171627998352\n",
      "[4/25][5/16] Loss_D: 0.1679 Loss_G: 6.8939\n",
      "1.4004151821136475\n",
      "[4/25][6/16] Loss_D: 1.4004 Loss_G: 22.9965\n",
      "0.8455639481544495\n",
      "[4/25][7/16] Loss_D: 0.8456 Loss_G: 25.2481\n",
      "0.6326931715011597\n",
      "[4/25][8/16] Loss_D: 0.6327 Loss_G: 21.6071\n",
      "0.2787958085536957\n",
      "[4/25][9/16] Loss_D: 0.2788 Loss_G: 14.6499\n",
      "0.12063825875520706\n",
      "[4/25][10/16] Loss_D: 0.1206 Loss_G: 6.5881\n",
      "0.7146761417388916\n",
      "[4/25][11/16] Loss_D: 0.7147 Loss_G: 14.4076\n",
      "0.242343470454216\n",
      "[4/25][12/16] Loss_D: 0.2423 Loss_G: 15.2689\n",
      "0.3833009898662567\n",
      "[4/25][13/16] Loss_D: 0.3833 Loss_G: 11.9867\n",
      "0.22632728517055511\n",
      "[4/25][14/16] Loss_D: 0.2263 Loss_G: 6.2095\n",
      "0.5288842916488647\n",
      "[4/25][15/16] Loss_D: 0.5289 Loss_G: 11.9554\n",
      "0.2238634079694748\n",
      "[5/25][0/16] Loss_D: 0.2239 Loss_G: 11.7951\n",
      "0.11139464378356934\n",
      "[5/25][1/16] Loss_D: 0.1114 Loss_G: 7.6656\n",
      "0.2622813880443573\n",
      "[5/25][2/16] Loss_D: 0.2623 Loss_G: 7.5176\n",
      "0.31879210472106934\n",
      "[5/25][3/16] Loss_D: 0.3188 Loss_G: 8.8539\n",
      "0.19035473465919495\n",
      "[5/25][4/16] Loss_D: 0.1904 Loss_G: 7.2848\n",
      "0.25729554891586304\n",
      "[5/25][5/16] Loss_D: 0.2573 Loss_G: 7.5052\n",
      "0.37758785486221313\n",
      "[5/25][6/16] Loss_D: 0.3776 Loss_G: 9.1127\n",
      "0.1323503702878952\n",
      "[5/25][7/16] Loss_D: 0.1324 Loss_G: 7.9374\n",
      "0.39511987566947937\n",
      "[5/25][8/16] Loss_D: 0.3951 Loss_G: 5.3983\n",
      "0.7456265687942505\n",
      "[5/25][9/16] Loss_D: 0.7456 Loss_G: 17.3788\n",
      "1.5952221155166626\n",
      "[5/25][10/16] Loss_D: 1.5952 Loss_G: 15.8620\n",
      "0.31583958864212036\n",
      "[5/25][11/16] Loss_D: 0.3158 Loss_G: 11.1625\n",
      "0.2561223804950714\n",
      "[5/25][12/16] Loss_D: 0.2561 Loss_G: 5.7942\n",
      "0.9386917352676392\n",
      "[5/25][13/16] Loss_D: 0.9387 Loss_G: 15.1151\n",
      "0.5443886518478394\n",
      "[5/25][14/16] Loss_D: 0.5444 Loss_G: 15.9997\n",
      "0.5725498199462891\n",
      "[5/25][15/16] Loss_D: 0.5725 Loss_G: 12.2940\n",
      "0.3415128290653229\n",
      "[6/25][0/16] Loss_D: 0.3415 Loss_G: 6.8843\n",
      "0.15281742811203003\n",
      "[6/25][1/16] Loss_D: 0.1528 Loss_G: 4.5226\n",
      "0.44855377078056335\n",
      "[6/25][2/16] Loss_D: 0.4486 Loss_G: 9.1317\n",
      "0.20372943580150604\n",
      "[6/25][3/16] Loss_D: 0.2037 Loss_G: 8.7644\n",
      "0.24965476989746094\n",
      "[6/25][4/16] Loss_D: 0.2497 Loss_G: 6.6505\n",
      "0.48308107256889343\n",
      "[6/25][5/16] Loss_D: 0.4831 Loss_G: 3.8772\n",
      "1.2116100788116455\n",
      "[6/25][6/16] Loss_D: 1.2116 Loss_G: 11.0272\n",
      "1.0639106035232544\n",
      "[6/25][7/16] Loss_D: 1.0639 Loss_G: 9.4434\n",
      "0.6807495355606079\n",
      "[6/25][8/16] Loss_D: 0.6807 Loss_G: 3.6814\n",
      "2.1766183376312256\n",
      "[6/25][9/16] Loss_D: 2.1766 Loss_G: 8.4155\n",
      "1.5392959117889404\n",
      "[6/25][10/16] Loss_D: 1.5393 Loss_G: 7.1779\n",
      "0.5582427978515625\n",
      "[6/25][11/16] Loss_D: 0.5582 Loss_G: 4.0315\n",
      "0.8205361366271973\n",
      "[6/25][12/16] Loss_D: 0.8205 Loss_G: 6.3406\n",
      "0.31653672456741333\n",
      "[6/25][13/16] Loss_D: 0.3165 Loss_G: 5.8347\n",
      "0.5180634260177612\n",
      "[6/25][14/16] Loss_D: 0.5181 Loss_G: 4.0020\n",
      "0.5238381624221802\n",
      "[6/25][15/16] Loss_D: 0.5238 Loss_G: 4.0010\n",
      "0.3914071023464203\n",
      "[7/25][0/16] Loss_D: 0.3914 Loss_G: 5.1036\n",
      "0.4479357600212097\n",
      "[7/25][1/16] Loss_D: 0.4479 Loss_G: 3.9027\n",
      "0.35736486315727234\n",
      "[7/25][2/16] Loss_D: 0.3574 Loss_G: 4.2139\n",
      "0.41195911169052124\n",
      "[7/25][3/16] Loss_D: 0.4120 Loss_G: 3.8934\n",
      "0.42016321420669556\n",
      "[7/25][4/16] Loss_D: 0.4202 Loss_G: 4.0436\n",
      "0.4863717555999756\n",
      "[7/25][5/16] Loss_D: 0.4864 Loss_G: 3.1181\n",
      "0.5131288766860962\n",
      "[7/25][6/16] Loss_D: 0.5131 Loss_G: 5.6723\n",
      "0.6980098485946655\n",
      "[7/25][7/16] Loss_D: 0.6980 Loss_G: 2.4959\n",
      "0.5420575737953186\n",
      "[7/25][8/16] Loss_D: 0.5421 Loss_G: 5.1327\n",
      "0.42833060026168823\n",
      "[7/25][9/16] Loss_D: 0.4283 Loss_G: 3.6083\n",
      "0.386618435382843\n",
      "[7/25][10/16] Loss_D: 0.3866 Loss_G: 2.7114\n",
      "0.68546462059021\n",
      "[7/25][11/16] Loss_D: 0.6855 Loss_G: 6.2423\n",
      "1.087623119354248\n",
      "[7/25][12/16] Loss_D: 1.0876 Loss_G: 2.1160\n",
      "0.6052185297012329\n",
      "[7/25][13/16] Loss_D: 0.6052 Loss_G: 4.6212\n",
      "0.32430946826934814\n",
      "[7/25][14/16] Loss_D: 0.3243 Loss_G: 4.0520\n",
      "0.5846377611160278\n",
      "[7/25][15/16] Loss_D: 0.5846 Loss_G: 2.0963\n",
      "1.1717214584350586\n",
      "[8/25][0/16] Loss_D: 1.1717 Loss_G: 7.4012\n",
      "1.0984644889831543\n",
      "[8/25][1/16] Loss_D: 1.0985 Loss_G: 5.0307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3475036919116974\n",
      "[8/25][2/16] Loss_D: 0.3475 Loss_G: 2.5140\n",
      "0.8824430108070374\n",
      "[8/25][3/16] Loss_D: 0.8824 Loss_G: 4.1184\n",
      "0.5842315554618835\n",
      "[8/25][4/16] Loss_D: 0.5842 Loss_G: 3.9225\n",
      "0.7845903635025024\n",
      "[8/25][5/16] Loss_D: 0.7846 Loss_G: 3.1874\n",
      "0.5166934132575989\n",
      "[8/25][6/16] Loss_D: 0.5167 Loss_G: 3.9530\n",
      "0.7191891670227051\n",
      "[8/25][7/16] Loss_D: 0.7192 Loss_G: 3.4549\n",
      "0.43526291847229004\n",
      "[8/25][8/16] Loss_D: 0.4353 Loss_G: 3.6028\n",
      "0.5281164646148682\n",
      "[8/25][9/16] Loss_D: 0.5281 Loss_G: 4.3084\n",
      "0.5581501722335815\n",
      "[8/25][10/16] Loss_D: 0.5582 Loss_G: 2.5294\n",
      "0.659511148929596\n",
      "[8/25][11/16] Loss_D: 0.6595 Loss_G: 5.2526\n",
      "0.5985652208328247\n",
      "[8/25][12/16] Loss_D: 0.5986 Loss_G: 2.7244\n",
      "0.539326548576355\n",
      "[8/25][13/16] Loss_D: 0.5393 Loss_G: 3.1672\n",
      "0.7543027400970459\n",
      "[8/25][14/16] Loss_D: 0.7543 Loss_G: 6.6987\n",
      "1.6776924133300781\n",
      "[8/25][15/16] Loss_D: 1.6777 Loss_G: 2.0857\n",
      "0.9891696572303772\n",
      "[9/25][0/16] Loss_D: 0.9892 Loss_G: 4.6765\n",
      "0.3572707772254944\n",
      "[9/25][1/16] Loss_D: 0.3573 Loss_G: 4.6203\n",
      "0.3584321141242981\n",
      "[9/25][2/16] Loss_D: 0.3584 Loss_G: 3.2230\n",
      "0.4723412096500397\n",
      "[9/25][3/16] Loss_D: 0.4723 Loss_G: 2.9264\n",
      "0.7584401965141296\n",
      "[9/25][4/16] Loss_D: 0.7584 Loss_G: 5.5958\n",
      "1.1747852563858032\n",
      "[9/25][5/16] Loss_D: 1.1748 Loss_G: 2.2236\n",
      "0.8063387274742126\n",
      "[9/25][6/16] Loss_D: 0.8063 Loss_G: 4.4358\n",
      "0.6523997783660889\n",
      "[9/25][7/16] Loss_D: 0.6524 Loss_G: 3.3034\n",
      "0.8392606973648071\n",
      "[9/25][8/16] Loss_D: 0.8393 Loss_G: 4.1349\n",
      "0.7914279699325562\n",
      "[9/25][9/16] Loss_D: 0.7914 Loss_G: 2.9308\n",
      "0.8426918983459473\n",
      "[9/25][10/16] Loss_D: 0.8427 Loss_G: 3.6332\n",
      "0.5544379949569702\n",
      "[9/25][11/16] Loss_D: 0.5544 Loss_G: 3.6183\n",
      "0.6459481120109558\n",
      "[9/25][12/16] Loss_D: 0.6459 Loss_G: 4.2341\n",
      "0.6146929264068604\n",
      "[9/25][13/16] Loss_D: 0.6147 Loss_G: 3.6701\n",
      "0.6815415024757385\n",
      "[9/25][14/16] Loss_D: 0.6815 Loss_G: 4.7043\n",
      "0.7876813411712646\n",
      "[9/25][15/16] Loss_D: 0.7877 Loss_G: 2.2668\n",
      "1.3955973386764526\n",
      "[10/25][0/16] Loss_D: 1.3956 Loss_G: 7.0556\n",
      "1.2477493286132812\n",
      "[10/25][1/16] Loss_D: 1.2477 Loss_G: 4.1321\n",
      "0.7690088748931885\n",
      "[10/25][2/16] Loss_D: 0.7690 Loss_G: 2.0122\n",
      "0.9895750284194946\n",
      "[10/25][3/16] Loss_D: 0.9896 Loss_G: 6.2909\n",
      "0.5838280320167542\n",
      "[10/25][4/16] Loss_D: 0.5838 Loss_G: 4.4897\n",
      "0.41415315866470337\n",
      "[10/25][5/16] Loss_D: 0.4142 Loss_G: 2.9549\n",
      "0.6689826250076294\n",
      "[10/25][6/16] Loss_D: 0.6690 Loss_G: 5.3403\n",
      "0.7982966303825378\n",
      "[10/25][7/16] Loss_D: 0.7983 Loss_G: 3.2727\n",
      "0.8741434812545776\n",
      "[10/25][8/16] Loss_D: 0.8741 Loss_G: 5.0377\n",
      "0.5638579726219177\n",
      "[10/25][9/16] Loss_D: 0.5639 Loss_G: 3.8583\n",
      "0.6708766222000122\n",
      "[10/25][10/16] Loss_D: 0.6709 Loss_G: 3.8221\n",
      "0.5316333770751953\n",
      "[10/25][11/16] Loss_D: 0.5316 Loss_G: 5.9847\n",
      "0.7100667357444763\n",
      "[10/25][12/16] Loss_D: 0.7101 Loss_G: 2.8926\n",
      "0.9280024766921997\n",
      "[10/25][13/16] Loss_D: 0.9280 Loss_G: 5.2535\n",
      "0.4938333034515381\n",
      "[10/25][14/16] Loss_D: 0.4938 Loss_G: 4.0545\n",
      "0.8123573660850525\n",
      "[10/25][15/16] Loss_D: 0.8124 Loss_G: 4.5205\n",
      "0.544180154800415\n",
      "[11/25][0/16] Loss_D: 0.5442 Loss_G: 3.4262\n",
      "0.8792277574539185\n",
      "[11/25][1/16] Loss_D: 0.8792 Loss_G: 3.9517\n",
      "0.9875452518463135\n",
      "[11/25][2/16] Loss_D: 0.9875 Loss_G: 2.1181\n",
      "0.8515913486480713\n",
      "[11/25][3/16] Loss_D: 0.8516 Loss_G: 4.9985\n",
      "0.5578194260597229\n",
      "[11/25][4/16] Loss_D: 0.5578 Loss_G: 3.5229\n",
      "0.4777395725250244\n",
      "[11/25][5/16] Loss_D: 0.4777 Loss_G: 4.2176\n",
      "0.6111572980880737\n",
      "[11/25][6/16] Loss_D: 0.6112 Loss_G: 2.8057\n",
      "0.9001972079277039\n",
      "[11/25][7/16] Loss_D: 0.9002 Loss_G: 6.8322\n",
      "1.5265194177627563\n",
      "[11/25][8/16] Loss_D: 1.5265 Loss_G: 1.7894\n",
      "1.5105046033859253\n",
      "[11/25][9/16] Loss_D: 1.5105 Loss_G: 6.5043\n",
      "0.8024418950080872\n",
      "[11/25][10/16] Loss_D: 0.8024 Loss_G: 4.0311\n",
      "0.5345361828804016\n",
      "[11/25][11/16] Loss_D: 0.5345 Loss_G: 3.3452\n",
      "0.8600114583969116\n",
      "[11/25][12/16] Loss_D: 0.8600 Loss_G: 5.7420\n",
      "0.9455554485321045\n",
      "[11/25][13/16] Loss_D: 0.9456 Loss_G: 2.4492\n",
      "1.4844603538513184\n",
      "[11/25][14/16] Loss_D: 1.4845 Loss_G: 6.8482\n",
      "1.2982679605484009\n",
      "[11/25][15/16] Loss_D: 1.2983 Loss_G: 2.5986\n",
      "0.8996250629425049\n",
      "[12/25][0/16] Loss_D: 0.8996 Loss_G: 3.9216\n",
      "0.5459822416305542\n",
      "[12/25][1/16] Loss_D: 0.5460 Loss_G: 4.4921\n",
      "0.3365490436553955\n",
      "[12/25][2/16] Loss_D: 0.3365 Loss_G: 3.6635\n",
      "0.5102945566177368\n",
      "[12/25][3/16] Loss_D: 0.5103 Loss_G: 4.2684\n",
      "0.3449926972389221\n",
      "[12/25][4/16] Loss_D: 0.3450 Loss_G: 4.4907\n",
      "0.4311167001724243\n",
      "[12/25][5/16] Loss_D: 0.4311 Loss_G: 5.5202\n",
      "0.6010645031929016\n",
      "[12/25][6/16] Loss_D: 0.6011 Loss_G: 3.9249\n",
      "1.054663896560669\n",
      "[12/25][7/16] Loss_D: 1.0547 Loss_G: 7.8024\n",
      "1.3203186988830566\n",
      "[12/25][8/16] Loss_D: 1.3203 Loss_G: 3.3903\n",
      "1.5614006519317627\n",
      "[12/25][9/16] Loss_D: 1.5614 Loss_G: 6.2800\n",
      "0.8585112690925598\n",
      "[12/25][10/16] Loss_D: 0.8585 Loss_G: 4.4882\n",
      "0.4731207489967346\n",
      "[12/25][11/16] Loss_D: 0.4731 Loss_G: 3.9615\n",
      "0.4239436388015747\n",
      "[12/25][12/16] Loss_D: 0.4239 Loss_G: 5.5108\n",
      "0.4670296907424927\n",
      "[12/25][13/16] Loss_D: 0.4670 Loss_G: 4.6758\n",
      "0.46173155307769775\n",
      "[12/25][14/16] Loss_D: 0.4617 Loss_G: 5.2862\n",
      "0.5959634780883789\n",
      "[12/25][15/16] Loss_D: 0.5960 Loss_G: 5.5767\n",
      "0.4758663475513458\n",
      "[13/25][0/16] Loss_D: 0.4759 Loss_G: 3.5573\n",
      "0.7879408597946167\n",
      "[13/25][1/16] Loss_D: 0.7879 Loss_G: 6.8877\n",
      "0.9459315538406372\n",
      "[13/25][2/16] Loss_D: 0.9459 Loss_G: 3.5376\n",
      "0.42100557684898376\n",
      "[13/25][3/16] Loss_D: 0.4210 Loss_G: 5.0139\n",
      "0.34230560064315796\n",
      "[13/25][4/16] Loss_D: 0.3423 Loss_G: 5.1127\n",
      "0.42157721519470215\n",
      "[13/25][5/16] Loss_D: 0.4216 Loss_G: 4.3946\n",
      "0.5307443141937256\n",
      "[13/25][6/16] Loss_D: 0.5307 Loss_G: 5.9589\n",
      "0.354918897151947\n",
      "[13/25][7/16] Loss_D: 0.3549 Loss_G: 5.0443\n",
      "0.5223504900932312\n",
      "[13/25][8/16] Loss_D: 0.5224 Loss_G: 5.7903\n",
      "0.5055572390556335\n",
      "[13/25][9/16] Loss_D: 0.5056 Loss_G: 7.2527\n",
      "0.5521135330200195\n",
      "[13/25][10/16] Loss_D: 0.5521 Loss_G: 4.2333\n",
      "0.903886616230011\n",
      "[13/25][11/16] Loss_D: 0.9039 Loss_G: 7.0325\n",
      "0.5521492958068848\n",
      "[13/25][12/16] Loss_D: 0.5521 Loss_G: 4.1486\n",
      "0.7739583253860474\n",
      "[13/25][13/16] Loss_D: 0.7740 Loss_G: 7.9115\n",
      "1.2445623874664307\n",
      "[13/25][14/16] Loss_D: 1.2446 Loss_G: 3.2293\n",
      "0.9231599569320679\n",
      "[13/25][15/16] Loss_D: 0.9232 Loss_G: 8.0958\n",
      "0.14510682225227356\n",
      "[14/25][0/16] Loss_D: 0.1451 Loss_G: 6.9078\n",
      "0.17719978094100952\n",
      "[14/25][1/16] Loss_D: 0.1772 Loss_G: 3.9512\n",
      "0.7525134086608887\n",
      "[14/25][2/16] Loss_D: 0.7525 Loss_G: 8.1286\n",
      "0.7057596445083618\n",
      "[14/25][3/16] Loss_D: 0.7058 Loss_G: 4.5196\n",
      "0.6760811805725098\n",
      "[14/25][4/16] Loss_D: 0.6761 Loss_G: 3.2796\n",
      "1.0286427736282349\n",
      "[14/25][5/16] Loss_D: 1.0286 Loss_G: 6.0281\n",
      "0.5888827443122864\n",
      "[14/25][6/16] Loss_D: 0.5889 Loss_G: 4.6852\n",
      "0.4479861259460449\n",
      "[14/25][7/16] Loss_D: 0.4480 Loss_G: 4.0617\n",
      "0.3617367446422577\n",
      "[14/25][8/16] Loss_D: 0.3617 Loss_G: 5.2663\n",
      "0.2662312686443329\n",
      "[14/25][9/16] Loss_D: 0.2662 Loss_G: 4.9807\n",
      "0.18282082676887512\n",
      "[14/25][10/16] Loss_D: 0.1828 Loss_G: 4.8153\n",
      "0.24330288171768188\n",
      "[14/25][11/16] Loss_D: 0.2433 Loss_G: 5.0668\n",
      "0.2795998156070709\n",
      "[14/25][12/16] Loss_D: 0.2796 Loss_G: 5.0243\n",
      "0.2317029982805252\n",
      "[14/25][13/16] Loss_D: 0.2317 Loss_G: 5.2067\n",
      "0.39763253927230835\n",
      "[14/25][14/16] Loss_D: 0.3976 Loss_G: 3.8740\n",
      "0.6224992871284485\n",
      "[14/25][15/16] Loss_D: 0.6225 Loss_G: 7.6475\n",
      "0.7276454567909241\n",
      "[15/25][0/16] Loss_D: 0.7276 Loss_G: 4.8908\n",
      "0.2845410406589508\n",
      "[15/25][1/16] Loss_D: 0.2845 Loss_G: 4.8140\n",
      "0.2777818739414215\n",
      "[15/25][2/16] Loss_D: 0.2778 Loss_G: 5.7935\n",
      "0.23343107104301453\n",
      "[15/25][3/16] Loss_D: 0.2334 Loss_G: 4.9189\n",
      "0.45356684923171997\n",
      "[15/25][4/16] Loss_D: 0.4536 Loss_G: 5.1548\n",
      "0.5873916149139404\n",
      "[15/25][5/16] Loss_D: 0.5874 Loss_G: 5.8418\n",
      "0.31877827644348145\n",
      "[15/25][6/16] Loss_D: 0.3188 Loss_G: 5.0784\n",
      "0.5050230622291565\n",
      "[15/25][7/16] Loss_D: 0.5050 Loss_G: 5.4300\n",
      "0.6576446890830994\n",
      "[15/25][8/16] Loss_D: 0.6576 Loss_G: 4.5058\n",
      "0.6603446006774902\n",
      "[15/25][9/16] Loss_D: 0.6603 Loss_G: 7.6198\n",
      "0.26440873742103577\n",
      "[15/25][10/16] Loss_D: 0.2644 Loss_G: 6.6744\n",
      "0.34399890899658203\n",
      "[15/25][11/16] Loss_D: 0.3440 Loss_G: 4.0505\n",
      "0.5971197485923767\n",
      "[15/25][12/16] Loss_D: 0.5971 Loss_G: 6.7235\n",
      "0.4103420376777649\n",
      "[15/25][13/16] Loss_D: 0.4103 Loss_G: 5.2672\n",
      "0.3788464069366455\n",
      "[15/25][14/16] Loss_D: 0.3788 Loss_G: 5.7659\n",
      "0.47370120882987976\n",
      "[15/25][15/16] Loss_D: 0.4737 Loss_G: 5.9970\n",
      "0.47883254289627075\n",
      "[16/25][0/16] Loss_D: 0.4788 Loss_G: 4.9377\n",
      "0.644124448299408\n",
      "[16/25][1/16] Loss_D: 0.6441 Loss_G: 7.2281\n",
      "0.5687346458435059\n",
      "[16/25][2/16] Loss_D: 0.5687 Loss_G: 5.0624\n",
      "0.5462560057640076\n",
      "[16/25][3/16] Loss_D: 0.5463 Loss_G: 5.0249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48114854097366333\n",
      "[16/25][4/16] Loss_D: 0.4811 Loss_G: 6.5427\n",
      "0.2673068046569824\n",
      "[16/25][5/16] Loss_D: 0.2673 Loss_G: 5.4293\n",
      "0.3950144350528717\n",
      "[16/25][6/16] Loss_D: 0.3950 Loss_G: 7.3878\n",
      "0.3917817771434784\n",
      "[16/25][7/16] Loss_D: 0.3918 Loss_G: 5.0064\n",
      "0.598250150680542\n",
      "[16/25][8/16] Loss_D: 0.5983 Loss_G: 7.4784\n",
      "0.29429835081100464\n",
      "[16/25][9/16] Loss_D: 0.2943 Loss_G: 5.7244\n",
      "0.4989706575870514\n",
      "[16/25][10/16] Loss_D: 0.4990 Loss_G: 5.5096\n",
      "0.46076667308807373\n",
      "[16/25][11/16] Loss_D: 0.4608 Loss_G: 4.6022\n",
      "0.8036748170852661\n",
      "[16/25][12/16] Loss_D: 0.8037 Loss_G: 8.7196\n",
      "1.6378660202026367\n",
      "[16/25][13/16] Loss_D: 1.6379 Loss_G: 3.7874\n",
      "0.9753974676132202\n",
      "[16/25][14/16] Loss_D: 0.9754 Loss_G: 5.3684\n",
      "0.35321423411369324\n",
      "[16/25][15/16] Loss_D: 0.3532 Loss_G: 5.6954\n",
      "0.819686233997345\n",
      "[17/25][0/16] Loss_D: 0.8197 Loss_G: 4.7534\n",
      "0.5033535957336426\n",
      "[17/25][1/16] Loss_D: 0.5034 Loss_G: 6.1354\n",
      "0.7021579742431641\n",
      "[17/25][2/16] Loss_D: 0.7022 Loss_G: 6.0597\n",
      "0.5891249179840088\n",
      "[17/25][3/16] Loss_D: 0.5891 Loss_G: 4.9226\n",
      "0.8582842946052551\n",
      "[17/25][4/16] Loss_D: 0.8583 Loss_G: 8.9434\n",
      "1.0224721431732178\n",
      "[17/25][5/16] Loss_D: 1.0225 Loss_G: 4.8863\n",
      "0.5618058443069458\n",
      "[17/25][6/16] Loss_D: 0.5618 Loss_G: 5.2423\n",
      "0.7114027738571167\n",
      "[17/25][7/16] Loss_D: 0.7114 Loss_G: 6.2364\n",
      "1.2185730934143066\n",
      "[17/25][8/16] Loss_D: 1.2186 Loss_G: 4.2622\n",
      "0.8739680051803589\n",
      "[17/25][9/16] Loss_D: 0.8740 Loss_G: 6.5731\n",
      "0.85227370262146\n",
      "[17/25][10/16] Loss_D: 0.8523 Loss_G: 3.4145\n",
      "0.6233317852020264\n",
      "[17/25][11/16] Loss_D: 0.6233 Loss_G: 5.9758\n",
      "0.6310029029846191\n",
      "[17/25][12/16] Loss_D: 0.6310 Loss_G: 3.3638\n",
      "0.5232722759246826\n",
      "[17/25][13/16] Loss_D: 0.5233 Loss_G: 5.7389\n",
      "0.5452133417129517\n",
      "[17/25][14/16] Loss_D: 0.5452 Loss_G: 2.8554\n",
      "0.7213701605796814\n",
      "[17/25][15/16] Loss_D: 0.7214 Loss_G: 6.5404\n",
      "0.5207439064979553\n",
      "[18/25][0/16] Loss_D: 0.5207 Loss_G: 4.2539\n",
      "0.5818208456039429\n",
      "[18/25][1/16] Loss_D: 0.5818 Loss_G: 3.5734\n",
      "0.5881844758987427\n",
      "[18/25][2/16] Loss_D: 0.5882 Loss_G: 5.6011\n",
      "0.7870224118232727\n",
      "[18/25][3/16] Loss_D: 0.7870 Loss_G: 1.9576\n",
      "1.0475845336914062\n",
      "[18/25][4/16] Loss_D: 1.0476 Loss_G: 7.3539\n",
      "1.3404755592346191\n",
      "[18/25][5/16] Loss_D: 1.3405 Loss_G: 2.5440\n",
      "1.9515854120254517\n",
      "[18/25][6/16] Loss_D: 1.9516 Loss_G: 8.2212\n",
      "1.7203290462493896\n",
      "[18/25][7/16] Loss_D: 1.7203 Loss_G: 3.7139\n",
      "0.6244384050369263\n",
      "[18/25][8/16] Loss_D: 0.6244 Loss_G: 4.2446\n",
      "0.539368748664856\n",
      "[18/25][9/16] Loss_D: 0.5394 Loss_G: 5.5802\n",
      "0.46505826711654663\n",
      "[18/25][10/16] Loss_D: 0.4651 Loss_G: 4.1656\n",
      "0.5023713111877441\n",
      "[18/25][11/16] Loss_D: 0.5024 Loss_G: 3.5752\n",
      "0.9433945417404175\n",
      "[18/25][12/16] Loss_D: 0.9434 Loss_G: 5.5226\n",
      "0.4976528286933899\n",
      "[18/25][13/16] Loss_D: 0.4977 Loss_G: 4.3372\n",
      "0.4836340546607971\n",
      "[18/25][14/16] Loss_D: 0.4836 Loss_G: 3.7603\n",
      "0.8650445938110352\n",
      "[18/25][15/16] Loss_D: 0.8650 Loss_G: 4.9028\n",
      "0.6783609390258789\n",
      "[19/25][0/16] Loss_D: 0.6784 Loss_G: 3.3437\n",
      "0.8384385108947754\n",
      "[19/25][1/16] Loss_D: 0.8384 Loss_G: 6.8646\n",
      "0.7270132303237915\n",
      "[19/25][2/16] Loss_D: 0.7270 Loss_G: 3.5583\n",
      "0.9080315828323364\n",
      "[19/25][3/16] Loss_D: 0.9080 Loss_G: 3.8527\n",
      "0.48498040437698364\n",
      "[19/25][4/16] Loss_D: 0.4850 Loss_G: 5.2516\n",
      "0.2802410125732422\n",
      "[19/25][5/16] Loss_D: 0.2802 Loss_G: 4.3843\n",
      "0.522127628326416\n",
      "[19/25][6/16] Loss_D: 0.5221 Loss_G: 3.7408\n",
      "0.7552402019500732\n",
      "[19/25][7/16] Loss_D: 0.7552 Loss_G: 5.8345\n",
      "0.933167576789856\n",
      "[19/25][8/16] Loss_D: 0.9332 Loss_G: 2.7826\n",
      "1.1216037273406982\n",
      "[19/25][9/16] Loss_D: 1.1216 Loss_G: 7.9850\n",
      "1.2647994756698608\n",
      "[19/25][10/16] Loss_D: 1.2648 Loss_G: 3.8402\n",
      "0.5600901246070862\n",
      "[19/25][11/16] Loss_D: 0.5601 Loss_G: 4.0322\n",
      "0.6701011657714844\n",
      "[19/25][12/16] Loss_D: 0.6701 Loss_G: 5.7349\n",
      "0.41799262166023254\n",
      "[19/25][13/16] Loss_D: 0.4180 Loss_G: 4.0366\n",
      "0.5071569681167603\n",
      "[19/25][14/16] Loss_D: 0.5072 Loss_G: 4.0003\n",
      "0.5439987182617188\n",
      "[19/25][15/16] Loss_D: 0.5440 Loss_G: 6.0605\n",
      "0.46961554884910583\n",
      "[20/25][0/16] Loss_D: 0.4696 Loss_G: 4.0864\n",
      "0.6265825033187866\n",
      "[20/25][1/16] Loss_D: 0.6266 Loss_G: 4.0030\n",
      "0.44625139236450195\n",
      "[20/25][2/16] Loss_D: 0.4463 Loss_G: 4.0310\n",
      "0.3613484799861908\n",
      "[20/25][3/16] Loss_D: 0.3613 Loss_G: 3.6324\n",
      "0.4034889042377472\n",
      "[20/25][4/16] Loss_D: 0.4035 Loss_G: 4.7428\n",
      "0.24515600502490997\n",
      "[20/25][5/16] Loss_D: 0.2452 Loss_G: 4.0879\n",
      "0.3424952030181885\n",
      "[20/25][6/16] Loss_D: 0.3425 Loss_G: 3.8436\n",
      "0.2384847104549408\n",
      "[20/25][7/16] Loss_D: 0.2385 Loss_G: 4.7932\n",
      "0.4114976227283478\n",
      "[20/25][8/16] Loss_D: 0.4115 Loss_G: 3.2146\n",
      "0.45525601506233215\n",
      "[20/25][9/16] Loss_D: 0.4553 Loss_G: 5.8602\n",
      "0.4732680320739746\n",
      "[20/25][10/16] Loss_D: 0.4733 Loss_G: 3.7103\n",
      "0.339021772146225\n",
      "[20/25][11/16] Loss_D: 0.3390 Loss_G: 5.1540\n",
      "0.5045253038406372\n",
      "[20/25][12/16] Loss_D: 0.5045 Loss_G: 3.2064\n",
      "0.5536361932754517\n",
      "[20/25][13/16] Loss_D: 0.5536 Loss_G: 6.6178\n",
      "0.5788877606391907\n",
      "[20/25][14/16] Loss_D: 0.5789 Loss_G: 4.1883\n",
      "0.44946354627609253\n",
      "[20/25][15/16] Loss_D: 0.4495 Loss_G: 4.0951\n",
      "0.21291367709636688\n",
      "[21/25][0/16] Loss_D: 0.2129 Loss_G: 4.5438\n",
      "0.3017035126686096\n",
      "[21/25][1/16] Loss_D: 0.3017 Loss_G: 5.9162\n",
      "0.3142213821411133\n",
      "[21/25][2/16] Loss_D: 0.3142 Loss_G: 4.5834\n",
      "0.3063387870788574\n",
      "[21/25][3/16] Loss_D: 0.3063 Loss_G: 4.2740\n",
      "0.4294971525669098\n",
      "[21/25][4/16] Loss_D: 0.4295 Loss_G: 6.0406\n",
      "0.5531135201454163\n",
      "[21/25][5/16] Loss_D: 0.5531 Loss_G: 3.4100\n",
      "0.6766318082809448\n",
      "[21/25][6/16] Loss_D: 0.6766 Loss_G: 6.9929\n",
      "0.8509195446968079\n",
      "[21/25][7/16] Loss_D: 0.8509 Loss_G: 1.8891\n",
      "0.820427656173706\n",
      "[21/25][8/16] Loss_D: 0.8204 Loss_G: 7.9004\n",
      "0.33160918951034546\n",
      "[21/25][9/16] Loss_D: 0.3316 Loss_G: 6.2112\n",
      "0.23763743042945862\n",
      "[21/25][10/16] Loss_D: 0.2376 Loss_G: 4.2225\n",
      "0.3424839675426483\n",
      "[21/25][11/16] Loss_D: 0.3425 Loss_G: 5.4805\n",
      "0.20531801879405975\n",
      "[21/25][12/16] Loss_D: 0.2053 Loss_G: 4.7486\n",
      "0.25367188453674316\n",
      "[21/25][13/16] Loss_D: 0.2537 Loss_G: 4.9382\n",
      "0.3254033923149109\n",
      "[21/25][14/16] Loss_D: 0.3254 Loss_G: 6.2655\n",
      "0.4648304283618927\n",
      "[21/25][15/16] Loss_D: 0.4648 Loss_G: 4.5080\n",
      "0.2410060167312622\n",
      "[22/25][0/16] Loss_D: 0.2410 Loss_G: 4.4995\n",
      "0.30151307582855225\n",
      "[22/25][1/16] Loss_D: 0.3015 Loss_G: 5.3529\n",
      "0.21614009141921997\n",
      "[22/25][2/16] Loss_D: 0.2161 Loss_G: 5.0380\n",
      "0.19374462962150574\n",
      "[22/25][3/16] Loss_D: 0.1937 Loss_G: 4.9532\n",
      "0.32997190952301025\n",
      "[22/25][4/16] Loss_D: 0.3300 Loss_G: 4.6192\n",
      "0.3736957907676697\n",
      "[22/25][5/16] Loss_D: 0.3737 Loss_G: 4.6809\n",
      "0.2849929928779602\n",
      "[22/25][6/16] Loss_D: 0.2850 Loss_G: 5.0224\n",
      "0.26609423756599426\n",
      "[22/25][7/16] Loss_D: 0.2661 Loss_G: 4.9781\n",
      "0.3123786747455597\n",
      "[22/25][8/16] Loss_D: 0.3124 Loss_G: 5.0963\n",
      "0.2211676687002182\n",
      "[22/25][9/16] Loss_D: 0.2212 Loss_G: 5.7325\n",
      "0.2280881553888321\n",
      "[22/25][10/16] Loss_D: 0.2281 Loss_G: 5.1440\n",
      "0.24868641793727875\n",
      "[22/25][11/16] Loss_D: 0.2487 Loss_G: 4.7094\n",
      "0.4417572021484375\n",
      "[22/25][12/16] Loss_D: 0.4418 Loss_G: 7.8346\n",
      "0.45828285813331604\n",
      "[22/25][13/16] Loss_D: 0.4583 Loss_G: 5.7644\n",
      "0.24115942418575287\n",
      "[22/25][14/16] Loss_D: 0.2412 Loss_G: 5.5990\n",
      "0.3376273214817047\n",
      "[22/25][15/16] Loss_D: 0.3376 Loss_G: 5.6729\n",
      "0.19924122095108032\n",
      "[23/25][0/16] Loss_D: 0.1992 Loss_G: 6.9564\n",
      "0.20575065910816193\n",
      "[23/25][1/16] Loss_D: 0.2058 Loss_G: 6.0339\n",
      "0.2843335270881653\n",
      "[23/25][2/16] Loss_D: 0.2843 Loss_G: 5.2487\n",
      "0.4168822467327118\n",
      "[23/25][3/16] Loss_D: 0.4169 Loss_G: 9.5013\n",
      "0.19067874550819397\n",
      "[23/25][4/16] Loss_D: 0.1907 Loss_G: 8.2993\n",
      "0.17704731225967407\n",
      "[23/25][5/16] Loss_D: 0.1770 Loss_G: 5.8881\n",
      "0.6468881964683533\n",
      "[23/25][6/16] Loss_D: 0.6469 Loss_G: 10.5747\n",
      "0.5720462203025818\n",
      "[23/25][7/16] Loss_D: 0.5720 Loss_G: 8.7961\n",
      "0.14729663729667664\n",
      "[23/25][8/16] Loss_D: 0.1473 Loss_G: 4.4470\n",
      "1.017669677734375\n",
      "[23/25][9/16] Loss_D: 1.0177 Loss_G: 10.7894\n",
      "1.0362197160720825\n",
      "[23/25][10/16] Loss_D: 1.0362 Loss_G: 6.4175\n",
      "0.7137069702148438\n",
      "[23/25][11/16] Loss_D: 0.7137 Loss_G: 8.4125\n",
      "0.2883084714412689\n",
      "[23/25][12/16] Loss_D: 0.2883 Loss_G: 7.0790\n",
      "0.4413411319255829\n",
      "[23/25][13/16] Loss_D: 0.4413 Loss_G: 5.5920\n",
      "0.32450711727142334\n",
      "[23/25][14/16] Loss_D: 0.3245 Loss_G: 5.2288\n",
      "0.21804414689540863\n",
      "[23/25][15/16] Loss_D: 0.2180 Loss_G: 5.4891\n",
      "0.1805456578731537\n",
      "[24/25][0/16] Loss_D: 0.1805 Loss_G: 5.6324\n",
      "0.22125889360904694\n",
      "[24/25][1/16] Loss_D: 0.2213 Loss_G: 5.1685\n",
      "0.19577142596244812\n",
      "[24/25][2/16] Loss_D: 0.1958 Loss_G: 4.7540\n",
      "0.25791051983833313\n",
      "[24/25][3/16] Loss_D: 0.2579 Loss_G: 5.1908\n",
      "0.24664683640003204\n",
      "[24/25][4/16] Loss_D: 0.2466 Loss_G: 4.6345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20669963955879211\n",
      "[24/25][5/16] Loss_D: 0.2067 Loss_G: 4.8639\n",
      "0.2696529030799866\n",
      "[24/25][6/16] Loss_D: 0.2697 Loss_G: 6.4988\n",
      "0.340154230594635\n",
      "[24/25][7/16] Loss_D: 0.3402 Loss_G: 5.0915\n",
      "0.3480527400970459\n",
      "[24/25][8/16] Loss_D: 0.3481 Loss_G: 5.4434\n",
      "0.5267242193222046\n",
      "[24/25][9/16] Loss_D: 0.5267 Loss_G: 9.2399\n",
      "0.8690812587738037\n",
      "[24/25][10/16] Loss_D: 0.8691 Loss_G: 4.8714\n",
      "0.8137472867965698\n",
      "[24/25][11/16] Loss_D: 0.8137 Loss_G: 10.3250\n",
      "0.3280889093875885\n",
      "[24/25][12/16] Loss_D: 0.3281 Loss_G: 9.3104\n",
      "0.21172365546226501\n",
      "[24/25][13/16] Loss_D: 0.2117 Loss_G: 5.7681\n",
      "0.8752583861351013\n",
      "[24/25][14/16] Loss_D: 0.8753 Loss_G: 8.4239\n",
      "0.4076743721961975\n",
      "[24/25][15/16] Loss_D: 0.4077 Loss_G: 7.0231\n",
      "0.3584781289100647\n",
      "[25/25][0/16] Loss_D: 0.3585 Loss_G: 5.2841\n",
      "0.5392645597457886\n",
      "[25/25][1/16] Loss_D: 0.5393 Loss_G: 8.1527\n",
      "0.3025188446044922\n",
      "[25/25][2/16] Loss_D: 0.3025 Loss_G: 7.4619\n",
      "0.4273218512535095\n",
      "[25/25][3/16] Loss_D: 0.4273 Loss_G: 5.5610\n",
      "1.026493787765503\n",
      "[25/25][4/16] Loss_D: 1.0265 Loss_G: 10.9045\n",
      "0.9007670283317566\n",
      "[25/25][5/16] Loss_D: 0.9008 Loss_G: 5.7983\n",
      "0.6790582537651062\n",
      "[25/25][6/16] Loss_D: 0.6791 Loss_G: 7.5250\n",
      "0.2826404571533203\n",
      "[25/25][7/16] Loss_D: 0.2826 Loss_G: 6.0158\n",
      "0.37162554264068604\n",
      "[25/25][8/16] Loss_D: 0.3716 Loss_G: 6.4327\n",
      "0.5346435308456421\n",
      "[25/25][9/16] Loss_D: 0.5346 Loss_G: 5.4836\n",
      "0.25188201665878296\n",
      "[25/25][10/16] Loss_D: 0.2519 Loss_G: 4.2566\n",
      "0.6953924894332886\n",
      "[25/25][11/16] Loss_D: 0.6954 Loss_G: 7.4565\n",
      "0.40202343463897705\n",
      "[25/25][12/16] Loss_D: 0.4020 Loss_G: 5.7706\n",
      "0.40234220027923584\n",
      "[25/25][13/16] Loss_D: 0.4023 Loss_G: 3.9635\n",
      "1.0872639417648315\n",
      "[25/25][14/16] Loss_D: 1.0873 Loss_G: 9.5795\n",
      "0.5297927260398865\n",
      "[25/25][15/16] Loss_D: 0.5298 Loss_G: 7.8865\n",
      "0.16104000806808472\n",
      "[26/25][0/16] Loss_D: 0.1610 Loss_G: 5.1079\n",
      "0.310910165309906\n",
      "[26/25][1/16] Loss_D: 0.3109 Loss_G: 3.8077\n",
      "0.8523712158203125\n",
      "[26/25][2/16] Loss_D: 0.8524 Loss_G: 8.3227\n",
      "0.4419470727443695\n",
      "[26/25][3/16] Loss_D: 0.4419 Loss_G: 6.6782\n",
      "0.2957608997821808\n",
      "[26/25][4/16] Loss_D: 0.2958 Loss_G: 3.6335\n",
      "0.637871265411377\n",
      "[26/25][5/16] Loss_D: 0.6379 Loss_G: 6.3563\n",
      "0.1591118425130844\n",
      "[26/25][6/16] Loss_D: 0.1591 Loss_G: 3.9899\n",
      "0.5734473466873169\n",
      "[26/25][7/16] Loss_D: 0.5734 Loss_G: 6.4046\n",
      "0.268625944852829\n",
      "[26/25][8/16] Loss_D: 0.2686 Loss_G: 4.8690\n",
      "0.23581194877624512\n",
      "[26/25][9/16] Loss_D: 0.2358 Loss_G: 3.6872\n",
      "0.4597839415073395\n",
      "[26/25][10/16] Loss_D: 0.4598 Loss_G: 4.9747\n",
      "0.3153802156448364\n",
      "[26/25][11/16] Loss_D: 0.3154 Loss_G: 5.0396\n",
      "0.43498262763023376\n",
      "[26/25][12/16] Loss_D: 0.4350 Loss_G: 2.8980\n",
      "0.821816086769104\n",
      "[26/25][13/16] Loss_D: 0.8218 Loss_G: 7.3414\n",
      "1.111814260482788\n",
      "[26/25][14/16] Loss_D: 1.1118 Loss_G: 1.9008\n",
      "1.1568591594696045\n",
      "[26/25][15/16] Loss_D: 1.1569 Loss_G: 9.1870\n",
      "0.5979178547859192\n",
      "[27/25][0/16] Loss_D: 0.5979 Loss_G: 4.2477\n",
      "0.3023691177368164\n",
      "[27/25][1/16] Loss_D: 0.3024 Loss_G: 3.9437\n",
      "0.33376747369766235\n",
      "[27/25][2/16] Loss_D: 0.3338 Loss_G: 7.3629\n",
      "0.3212134540081024\n",
      "[27/25][3/16] Loss_D: 0.3212 Loss_G: 6.0390\n",
      "0.23144850134849548\n",
      "[27/25][4/16] Loss_D: 0.2314 Loss_G: 3.8500\n",
      "0.3444552421569824\n",
      "[27/25][5/16] Loss_D: 0.3445 Loss_G: 3.3144\n",
      "0.2622471749782562\n",
      "[27/25][6/16] Loss_D: 0.2622 Loss_G: 4.8285\n",
      "0.16232332587242126\n",
      "[27/25][7/16] Loss_D: 0.1623 Loss_G: 4.6371\n",
      "0.15758086740970612\n",
      "[27/25][8/16] Loss_D: 0.1576 Loss_G: 4.0824\n",
      "0.10772593319416046\n",
      "[27/25][9/16] Loss_D: 0.1077 Loss_G: 4.0945\n",
      "0.35579589009284973\n",
      "[27/25][10/16] Loss_D: 0.3558 Loss_G: 3.4474\n",
      "0.26308128237724304\n",
      "[27/25][11/16] Loss_D: 0.2631 Loss_G: 4.7018\n",
      "0.17363132536411285\n",
      "[27/25][12/16] Loss_D: 0.1736 Loss_G: 4.6495\n",
      "0.36012911796569824\n",
      "[27/25][13/16] Loss_D: 0.3601 Loss_G: 3.4652\n",
      "0.1471116840839386\n",
      "[27/25][14/16] Loss_D: 0.1471 Loss_G: 4.4823\n",
      "0.29310211539268494\n",
      "[27/25][15/16] Loss_D: 0.2931 Loss_G: 5.2425\n",
      "0.1380387544631958\n",
      "[28/25][0/16] Loss_D: 0.1380 Loss_G: 5.4452\n",
      "0.2799074649810791\n",
      "[28/25][1/16] Loss_D: 0.2799 Loss_G: 4.3121\n",
      "0.27120548486709595\n",
      "[28/25][2/16] Loss_D: 0.2712 Loss_G: 4.9624\n",
      "0.31351393461227417\n",
      "[28/25][3/16] Loss_D: 0.3135 Loss_G: 5.1999\n",
      "0.3498065173625946\n",
      "[28/25][4/16] Loss_D: 0.3498 Loss_G: 4.7735\n",
      "0.4556835889816284\n",
      "[28/25][5/16] Loss_D: 0.4557 Loss_G: 5.4711\n",
      "0.7138379812240601\n",
      "[28/25][6/16] Loss_D: 0.7138 Loss_G: 6.6316\n",
      "0.48304271697998047\n",
      "[28/25][7/16] Loss_D: 0.4830 Loss_G: 5.0531\n",
      "0.42961248755455017\n",
      "[28/25][8/16] Loss_D: 0.4296 Loss_G: 6.4124\n",
      "0.4838947653770447\n",
      "[28/25][9/16] Loss_D: 0.4839 Loss_G: 6.2225\n",
      "0.5753782391548157\n",
      "[28/25][10/16] Loss_D: 0.5754 Loss_G: 4.2068\n",
      "0.4870871603488922\n",
      "[28/25][11/16] Loss_D: 0.4871 Loss_G: 4.8040\n",
      "0.23029837012290955\n",
      "[28/25][12/16] Loss_D: 0.2303 Loss_G: 5.3873\n",
      "0.2450847327709198\n",
      "[28/25][13/16] Loss_D: 0.2451 Loss_G: 4.6231\n",
      "0.27833566069602966\n",
      "[28/25][14/16] Loss_D: 0.2783 Loss_G: 4.5879\n",
      "0.3874984085559845\n",
      "[28/25][15/16] Loss_D: 0.3875 Loss_G: 6.4074\n",
      "0.22986236214637756\n",
      "[29/25][0/16] Loss_D: 0.2299 Loss_G: 5.5986\n",
      "0.6652324199676514\n",
      "[29/25][1/16] Loss_D: 0.6652 Loss_G: 7.4289\n",
      "0.7765992879867554\n",
      "[29/25][2/16] Loss_D: 0.7766 Loss_G: 4.2675\n",
      "0.579029381275177\n",
      "[29/25][3/16] Loss_D: 0.5790 Loss_G: 5.8248\n",
      "0.26429808139801025\n",
      "[29/25][4/16] Loss_D: 0.2643 Loss_G: 5.5654\n",
      "0.3640764355659485\n",
      "[29/25][5/16] Loss_D: 0.3641 Loss_G: 4.4292\n",
      "0.46699249744415283\n",
      "[29/25][6/16] Loss_D: 0.4670 Loss_G: 6.2626\n",
      "0.8214496970176697\n",
      "[29/25][7/16] Loss_D: 0.8214 Loss_G: 2.6213\n",
      "0.6624419689178467\n",
      "[29/25][8/16] Loss_D: 0.6624 Loss_G: 6.4002\n",
      "0.3441371023654938\n",
      "[29/25][9/16] Loss_D: 0.3441 Loss_G: 5.2695\n",
      "0.2949362099170685\n",
      "[29/25][10/16] Loss_D: 0.2949 Loss_G: 3.7191\n",
      "0.5876960754394531\n",
      "[29/25][11/16] Loss_D: 0.5877 Loss_G: 5.2947\n",
      "0.3096630573272705\n",
      "[29/25][12/16] Loss_D: 0.3097 Loss_G: 5.0991\n",
      "0.3150699734687805\n",
      "[29/25][13/16] Loss_D: 0.3151 Loss_G: 3.8259\n",
      "0.2873186767101288\n",
      "[29/25][14/16] Loss_D: 0.2873 Loss_G: 4.5136\n",
      "0.4237349033355713\n",
      "[29/25][15/16] Loss_D: 0.4237 Loss_G: 5.9090\n",
      "0.21912826597690582\n",
      "[30/25][0/16] Loss_D: 0.2191 Loss_G: 5.6747\n",
      "0.31863051652908325\n",
      "[30/25][1/16] Loss_D: 0.3186 Loss_G: 4.4785\n",
      "0.28479379415512085\n",
      "[30/25][2/16] Loss_D: 0.2848 Loss_G: 5.9649\n",
      "0.46580371260643005\n",
      "[30/25][3/16] Loss_D: 0.4658 Loss_G: 3.5210\n",
      "0.5253978967666626\n",
      "[30/25][4/16] Loss_D: 0.5254 Loss_G: 7.7106\n",
      "0.6013510823249817\n",
      "[30/25][5/16] Loss_D: 0.6014 Loss_G: 3.9193\n",
      "0.4727270007133484\n",
      "[30/25][6/16] Loss_D: 0.4727 Loss_G: 5.3729\n",
      "0.3039892315864563\n",
      "[30/25][7/16] Loss_D: 0.3040 Loss_G: 5.0107\n",
      "0.4978456199169159\n",
      "[30/25][8/16] Loss_D: 0.4978 Loss_G: 4.0024\n",
      "0.4667425751686096\n",
      "[30/25][9/16] Loss_D: 0.4667 Loss_G: 6.5259\n",
      "0.4837290346622467\n",
      "[30/25][10/16] Loss_D: 0.4837 Loss_G: 4.1315\n",
      "0.6524778604507446\n",
      "[30/25][11/16] Loss_D: 0.6525 Loss_G: 6.0122\n",
      "0.3168467879295349\n",
      "[30/25][12/16] Loss_D: 0.3168 Loss_G: 5.9607\n",
      "0.1895054429769516\n",
      "[30/25][13/16] Loss_D: 0.1895 Loss_G: 5.2710\n",
      "0.16210073232650757\n",
      "[30/25][14/16] Loss_D: 0.1621 Loss_G: 5.0305\n",
      "0.11289368569850922\n",
      "[30/25][15/16] Loss_D: 0.1129 Loss_G: 4.9707\n",
      "0.18413765728473663\n",
      "[31/25][0/16] Loss_D: 0.1841 Loss_G: 5.2032\n",
      "0.1353672444820404\n",
      "[31/25][1/16] Loss_D: 0.1354 Loss_G: 5.0517\n",
      "0.1488064080476761\n",
      "[31/25][2/16] Loss_D: 0.1488 Loss_G: 4.7348\n",
      "0.24537545442581177\n",
      "[31/25][3/16] Loss_D: 0.2454 Loss_G: 5.2392\n",
      "0.4180915653705597\n",
      "[31/25][4/16] Loss_D: 0.4181 Loss_G: 4.0793\n",
      "0.45594894886016846\n",
      "[31/25][5/16] Loss_D: 0.4559 Loss_G: 4.4249\n",
      "0.37772542238235474\n",
      "[31/25][6/16] Loss_D: 0.3777 Loss_G: 6.5288\n",
      "0.34989404678344727\n",
      "[31/25][7/16] Loss_D: 0.3499 Loss_G: 4.5069\n",
      "0.19713035225868225\n",
      "[31/25][8/16] Loss_D: 0.1971 Loss_G: 4.2247\n",
      "0.34367960691452026\n",
      "[31/25][9/16] Loss_D: 0.3437 Loss_G: 6.5358\n",
      "0.2513446509838104\n",
      "[31/25][10/16] Loss_D: 0.2513 Loss_G: 5.2971\n",
      "0.3156943917274475\n",
      "[31/25][11/16] Loss_D: 0.3157 Loss_G: 3.3502\n",
      "0.5497874021530151\n",
      "[31/25][12/16] Loss_D: 0.5498 Loss_G: 7.3188\n",
      "0.6516072154045105\n",
      "[31/25][13/16] Loss_D: 0.6516 Loss_G: 4.4552\n",
      "0.21919842064380646\n",
      "[31/25][14/16] Loss_D: 0.2192 Loss_G: 4.7231\n",
      "0.20919683575630188\n",
      "[31/25][15/16] Loss_D: 0.2092 Loss_G: 5.7142\n",
      "0.12226022779941559\n",
      "[32/25][0/16] Loss_D: 0.1223 Loss_G: 5.4042\n",
      "0.2456578016281128\n",
      "[32/25][1/16] Loss_D: 0.2457 Loss_G: 3.9768\n",
      "0.28473395109176636\n",
      "[32/25][2/16] Loss_D: 0.2847 Loss_G: 5.1145\n",
      "0.37148821353912354\n",
      "[32/25][3/16] Loss_D: 0.3715 Loss_G: 4.5844\n",
      "0.260247141122818\n",
      "[32/25][4/16] Loss_D: 0.2602 Loss_G: 4.2254\n",
      "0.2488189935684204\n",
      "[32/25][5/16] Loss_D: 0.2488 Loss_G: 4.7692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3198264241218567\n",
      "[32/25][6/16] Loss_D: 0.3198 Loss_G: 5.5680\n",
      "0.23266056180000305\n",
      "[32/25][7/16] Loss_D: 0.2327 Loss_G: 4.9792\n",
      "0.18849603831768036\n",
      "[32/25][8/16] Loss_D: 0.1885 Loss_G: 4.3688\n",
      "0.27926602959632874\n",
      "[32/25][9/16] Loss_D: 0.2793 Loss_G: 4.0320\n",
      "0.26650553941726685\n",
      "[32/25][10/16] Loss_D: 0.2665 Loss_G: 5.4302\n",
      "0.1280277967453003\n",
      "[32/25][11/16] Loss_D: 0.1280 Loss_G: 5.4791\n",
      "0.20891684293746948\n",
      "[32/25][12/16] Loss_D: 0.2089 Loss_G: 4.0920\n",
      "0.22876349091529846\n",
      "[32/25][13/16] Loss_D: 0.2288 Loss_G: 4.7187\n",
      "0.22206412255764008\n",
      "[32/25][14/16] Loss_D: 0.2221 Loss_G: 5.0260\n",
      "0.25759071111679077\n",
      "[32/25][15/16] Loss_D: 0.2576 Loss_G: 3.7600\n",
      "0.3884340524673462\n",
      "[33/25][0/16] Loss_D: 0.3884 Loss_G: 6.8945\n",
      "0.36875149607658386\n",
      "[33/25][1/16] Loss_D: 0.3688 Loss_G: 5.4881\n",
      "0.0865863636136055\n",
      "[33/25][2/16] Loss_D: 0.0866 Loss_G: 4.8139\n",
      "0.15018807351589203\n",
      "[33/25][3/16] Loss_D: 0.1502 Loss_G: 5.0015\n",
      "0.17963896691799164\n",
      "[33/25][4/16] Loss_D: 0.1796 Loss_G: 5.0010\n",
      "0.18634626269340515\n",
      "[33/25][5/16] Loss_D: 0.1863 Loss_G: 5.0174\n",
      "0.29634997248649597\n",
      "[33/25][6/16] Loss_D: 0.2963 Loss_G: 4.6191\n",
      "0.2791360020637512\n",
      "[33/25][7/16] Loss_D: 0.2791 Loss_G: 4.6994\n",
      "0.3974747657775879\n",
      "[33/25][8/16] Loss_D: 0.3975 Loss_G: 4.9104\n",
      "0.24723652005195618\n",
      "[33/25][9/16] Loss_D: 0.2472 Loss_G: 4.3593\n",
      "0.4895813465118408\n",
      "[33/25][10/16] Loss_D: 0.4896 Loss_G: 3.9852\n",
      "0.561125636100769\n",
      "[33/25][11/16] Loss_D: 0.5611 Loss_G: 4.1550\n",
      "0.285694420337677\n",
      "[33/25][12/16] Loss_D: 0.2857 Loss_G: 4.9736\n",
      "0.30428066849708557\n",
      "[33/25][13/16] Loss_D: 0.3043 Loss_G: 4.6815\n",
      "0.19130077958106995\n",
      "[33/25][14/16] Loss_D: 0.1913 Loss_G: 5.0392\n",
      "0.2740020155906677\n",
      "[33/25][15/16] Loss_D: 0.2740 Loss_G: 6.0137\n",
      "0.46346527338027954\n",
      "[34/25][0/16] Loss_D: 0.4635 Loss_G: 2.7876\n",
      "1.2966214418411255\n",
      "[34/25][1/16] Loss_D: 1.2966 Loss_G: 12.4209\n",
      "2.513202667236328\n",
      "[34/25][2/16] Loss_D: 2.5132 Loss_G: 5.9054\n",
      "0.27981358766555786\n",
      "[34/25][3/16] Loss_D: 0.2798 Loss_G: 4.5588\n",
      "0.31513890624046326\n",
      "[34/25][4/16] Loss_D: 0.3151 Loss_G: 5.3351\n",
      "0.20922189950942993\n",
      "[34/25][5/16] Loss_D: 0.2092 Loss_G: 5.6274\n",
      "0.36767733097076416\n",
      "[34/25][6/16] Loss_D: 0.3677 Loss_G: 5.5611\n",
      "0.37457937002182007\n",
      "[34/25][7/16] Loss_D: 0.3746 Loss_G: 4.7205\n",
      "0.5261741280555725\n",
      "[34/25][8/16] Loss_D: 0.5262 Loss_G: 5.3831\n",
      "0.3640434741973877\n",
      "[34/25][9/16] Loss_D: 0.3640 Loss_G: 4.3736\n",
      "0.6332498788833618\n",
      "[34/25][10/16] Loss_D: 0.6332 Loss_G: 5.9560\n",
      "0.23187443614006042\n",
      "[34/25][11/16] Loss_D: 0.2319 Loss_G: 5.6183\n",
      "0.23472654819488525\n",
      "[34/25][12/16] Loss_D: 0.2347 Loss_G: 5.1055\n",
      "0.2979761064052582\n",
      "[34/25][13/16] Loss_D: 0.2980 Loss_G: 5.8189\n",
      "0.4400583505630493\n",
      "[34/25][14/16] Loss_D: 0.4401 Loss_G: 6.2277\n",
      "0.7410163879394531\n",
      "[34/25][15/16] Loss_D: 0.7410 Loss_G: 4.5517\n",
      "0.5572859048843384\n",
      "[35/25][0/16] Loss_D: 0.5573 Loss_G: 6.5384\n",
      "0.28139230608940125\n",
      "[35/25][1/16] Loss_D: 0.2814 Loss_G: 5.7640\n",
      "0.1297033131122589\n",
      "[35/25][2/16] Loss_D: 0.1297 Loss_G: 4.9474\n",
      "0.2147064507007599\n",
      "[35/25][3/16] Loss_D: 0.2147 Loss_G: 5.0496\n",
      "0.18439681828022003\n",
      "[35/25][4/16] Loss_D: 0.1844 Loss_G: 5.0014\n",
      "0.19143950939178467\n",
      "[35/25][5/16] Loss_D: 0.1914 Loss_G: 5.1611\n",
      "0.18531976640224457\n",
      "[35/25][6/16] Loss_D: 0.1853 Loss_G: 4.7005\n",
      "0.24652239680290222\n",
      "[35/25][7/16] Loss_D: 0.2465 Loss_G: 4.8600\n",
      "0.21147511899471283\n",
      "[35/25][8/16] Loss_D: 0.2115 Loss_G: 5.0343\n",
      "0.21636389195919037\n",
      "[35/25][9/16] Loss_D: 0.2164 Loss_G: 5.6323\n",
      "0.28929567337036133\n",
      "[35/25][10/16] Loss_D: 0.2893 Loss_G: 5.3097\n",
      "0.14311081171035767\n",
      "[35/25][11/16] Loss_D: 0.1431 Loss_G: 4.9407\n",
      "0.42404139041900635\n",
      "[35/25][12/16] Loss_D: 0.4240 Loss_G: 5.1434\n",
      "0.3366236388683319\n",
      "[35/25][13/16] Loss_D: 0.3366 Loss_G: 4.1750\n",
      "0.31145721673965454\n",
      "[35/25][14/16] Loss_D: 0.3115 Loss_G: 4.0157\n",
      "0.3199504017829895\n",
      "[35/25][15/16] Loss_D: 0.3200 Loss_G: 6.4944\n",
      "0.5144994854927063\n",
      "[36/25][0/16] Loss_D: 0.5145 Loss_G: 3.7250\n",
      "0.3608887195587158\n",
      "[36/25][1/16] Loss_D: 0.3609 Loss_G: 6.1489\n",
      "0.37585291266441345\n",
      "[36/25][2/16] Loss_D: 0.3759 Loss_G: 4.0733\n",
      "0.2698274254798889\n",
      "[36/25][3/16] Loss_D: 0.2698 Loss_G: 3.6855\n",
      "0.49442315101623535\n",
      "[36/25][4/16] Loss_D: 0.4944 Loss_G: 8.0823\n",
      "0.669399619102478\n",
      "[36/25][5/16] Loss_D: 0.6694 Loss_G: 4.5396\n",
      "0.20832139253616333\n",
      "[36/25][6/16] Loss_D: 0.2083 Loss_G: 4.5233\n",
      "0.16011667251586914\n",
      "[36/25][7/16] Loss_D: 0.1601 Loss_G: 5.6590\n",
      "0.1796204000711441\n",
      "[36/25][8/16] Loss_D: 0.1796 Loss_G: 4.6064\n",
      "0.2611406445503235\n",
      "[36/25][9/16] Loss_D: 0.2611 Loss_G: 5.3373\n",
      "0.1849992275238037\n",
      "[36/25][10/16] Loss_D: 0.1850 Loss_G: 4.8831\n",
      "0.2398863434791565\n",
      "[36/25][11/16] Loss_D: 0.2399 Loss_G: 5.0212\n",
      "0.3580504059791565\n",
      "[36/25][12/16] Loss_D: 0.3581 Loss_G: 3.5513\n",
      "0.44758158922195435\n",
      "[36/25][13/16] Loss_D: 0.4476 Loss_G: 6.6104\n",
      "0.41299623250961304\n",
      "[36/25][14/16] Loss_D: 0.4130 Loss_G: 4.6306\n",
      "0.3422470688819885\n",
      "[36/25][15/16] Loss_D: 0.3422 Loss_G: 3.7070\n",
      "0.550558865070343\n",
      "[37/25][0/16] Loss_D: 0.5506 Loss_G: 8.3167\n",
      "0.3759985864162445\n",
      "[37/25][1/16] Loss_D: 0.3760 Loss_G: 6.7714\n",
      "0.29074302315711975\n",
      "[37/25][2/16] Loss_D: 0.2907 Loss_G: 3.9446\n",
      "0.6005869507789612\n",
      "[37/25][3/16] Loss_D: 0.6006 Loss_G: 6.7993\n",
      "0.3384731411933899\n",
      "[37/25][4/16] Loss_D: 0.3385 Loss_G: 4.8479\n",
      "0.2825430631637573\n",
      "[37/25][5/16] Loss_D: 0.2825 Loss_G: 5.0212\n",
      "0.3483940660953522\n",
      "[37/25][6/16] Loss_D: 0.3484 Loss_G: 4.8303\n",
      "0.23220139741897583\n",
      "[37/25][7/16] Loss_D: 0.2322 Loss_G: 4.3358\n",
      "0.37177714705467224\n",
      "[37/25][8/16] Loss_D: 0.3718 Loss_G: 6.4301\n",
      "0.4337310791015625\n",
      "[37/25][9/16] Loss_D: 0.4337 Loss_G: 3.5487\n",
      "0.3683481216430664\n",
      "[37/25][10/16] Loss_D: 0.3683 Loss_G: 6.4249\n",
      "0.33958715200424194\n",
      "[37/25][11/16] Loss_D: 0.3396 Loss_G: 4.4358\n",
      "0.3090042769908905\n",
      "[37/25][12/16] Loss_D: 0.3090 Loss_G: 6.2648\n",
      "0.21456143260002136\n",
      "[37/25][13/16] Loss_D: 0.2146 Loss_G: 5.3105\n",
      "0.2142474353313446\n",
      "[37/25][14/16] Loss_D: 0.2142 Loss_G: 4.8641\n",
      "0.20541802048683167\n",
      "[37/25][15/16] Loss_D: 0.2054 Loss_G: 5.4883\n",
      "0.17261409759521484\n",
      "[38/25][0/16] Loss_D: 0.1726 Loss_G: 5.6002\n",
      "0.2728042006492615\n",
      "[38/25][1/16] Loss_D: 0.2728 Loss_G: 4.2904\n",
      "0.29318127036094666\n",
      "[38/25][2/16] Loss_D: 0.2932 Loss_G: 5.2260\n",
      "0.2817174792289734\n",
      "[38/25][3/16] Loss_D: 0.2817 Loss_G: 5.0777\n",
      "0.3415592908859253\n",
      "[38/25][4/16] Loss_D: 0.3416 Loss_G: 4.8123\n",
      "0.5715548992156982\n",
      "[38/25][5/16] Loss_D: 0.5716 Loss_G: 6.5503\n",
      "0.23743362724781036\n",
      "[38/25][6/16] Loss_D: 0.2374 Loss_G: 5.8420\n",
      "0.17096658051013947\n",
      "[38/25][7/16] Loss_D: 0.1710 Loss_G: 4.4096\n",
      "0.32088786363601685\n",
      "[38/25][8/16] Loss_D: 0.3209 Loss_G: 7.0278\n",
      "0.31275662779808044\n",
      "[38/25][9/16] Loss_D: 0.3128 Loss_G: 4.8515\n",
      "0.2316817343235016\n",
      "[38/25][10/16] Loss_D: 0.2317 Loss_G: 4.2562\n",
      "0.30136626958847046\n",
      "[38/25][11/16] Loss_D: 0.3014 Loss_G: 6.4488\n",
      "0.17857231199741364\n",
      "[38/25][12/16] Loss_D: 0.1786 Loss_G: 5.9628\n",
      "0.12135675549507141\n",
      "[38/25][13/16] Loss_D: 0.1214 Loss_G: 4.4401\n",
      "0.2369508147239685\n",
      "[38/25][14/16] Loss_D: 0.2370 Loss_G: 5.0561\n",
      "0.18714158236980438\n",
      "[38/25][15/16] Loss_D: 0.1871 Loss_G: 4.8593\n",
      "0.21847712993621826\n",
      "[39/25][0/16] Loss_D: 0.2185 Loss_G: 5.8250\n",
      "0.2617059350013733\n",
      "[39/25][1/16] Loss_D: 0.2617 Loss_G: 4.5114\n",
      "0.38786354660987854\n",
      "[39/25][2/16] Loss_D: 0.3879 Loss_G: 6.0361\n",
      "0.2924576699733734\n",
      "[39/25][3/16] Loss_D: 0.2925 Loss_G: 4.7865\n",
      "0.23316802084445953\n",
      "[39/25][4/16] Loss_D: 0.2332 Loss_G: 6.3342\n",
      "0.17746640741825104\n",
      "[39/25][5/16] Loss_D: 0.1775 Loss_G: 5.4814\n",
      "0.21846604347229004\n",
      "[39/25][6/16] Loss_D: 0.2185 Loss_G: 3.9663\n",
      "0.3320382833480835\n",
      "[39/25][7/16] Loss_D: 0.3320 Loss_G: 7.4961\n",
      "0.3282780647277832\n",
      "[39/25][8/16] Loss_D: 0.3283 Loss_G: 5.2909\n",
      "0.22463491559028625\n",
      "[39/25][9/16] Loss_D: 0.2246 Loss_G: 4.0096\n",
      "0.451717734336853\n",
      "[39/25][10/16] Loss_D: 0.4517 Loss_G: 8.4425\n",
      "0.3454701602458954\n",
      "[39/25][11/16] Loss_D: 0.3455 Loss_G: 4.6141\n",
      "0.36080724000930786\n",
      "[39/25][12/16] Loss_D: 0.3608 Loss_G: 2.9317\n",
      "0.5274040102958679\n",
      "[39/25][13/16] Loss_D: 0.5274 Loss_G: 10.2698\n",
      "0.3372463881969452\n",
      "[39/25][14/16] Loss_D: 0.3372 Loss_G: 7.2198\n",
      "0.20278838276863098\n",
      "[39/25][15/16] Loss_D: 0.2028 Loss_G: 3.6015\n",
      "0.6311888694763184\n",
      "[40/25][0/16] Loss_D: 0.6312 Loss_G: 7.6256\n",
      "0.3074193596839905\n",
      "[40/25][1/16] Loss_D: 0.3074 Loss_G: 5.7675\n",
      "0.3939678370952606\n",
      "[40/25][2/16] Loss_D: 0.3940 Loss_G: 2.6606\n",
      "0.6561046838760376\n",
      "[40/25][3/16] Loss_D: 0.6561 Loss_G: 6.9696\n",
      "0.43728703260421753\n",
      "[40/25][4/16] Loss_D: 0.4373 Loss_G: 4.3710\n",
      "0.35039839148521423\n",
      "[40/25][5/16] Loss_D: 0.3504 Loss_G: 4.7445\n",
      "0.46512866020202637\n",
      "[40/25][6/16] Loss_D: 0.4651 Loss_G: 4.2941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18567807972431183\n",
      "[40/25][7/16] Loss_D: 0.1857 Loss_G: 5.3212\n",
      "0.3088928759098053\n",
      "[40/25][8/16] Loss_D: 0.3089 Loss_G: 3.7635\n",
      "0.3253611922264099\n",
      "[40/25][9/16] Loss_D: 0.3254 Loss_G: 6.4992\n",
      "0.3018002212047577\n",
      "[40/25][10/16] Loss_D: 0.3018 Loss_G: 5.1411\n",
      "0.32815659046173096\n",
      "[40/25][11/16] Loss_D: 0.3282 Loss_G: 3.5852\n",
      "0.3868691623210907\n",
      "[40/25][12/16] Loss_D: 0.3869 Loss_G: 5.8053\n",
      "0.16808289289474487\n",
      "[40/25][13/16] Loss_D: 0.1681 Loss_G: 5.6266\n",
      "0.1870923936367035\n",
      "[40/25][14/16] Loss_D: 0.1871 Loss_G: 4.4785\n",
      "0.17349253594875336\n",
      "[40/25][15/16] Loss_D: 0.1735 Loss_G: 4.0588\n",
      "0.4868246018886566\n",
      "[41/25][0/16] Loss_D: 0.4868 Loss_G: 7.3395\n",
      "0.4938415586948395\n",
      "[41/25][1/16] Loss_D: 0.4938 Loss_G: 4.7519\n",
      "0.23688867688179016\n",
      "[41/25][2/16] Loss_D: 0.2369 Loss_G: 3.6604\n",
      "0.2642250955104828\n",
      "[41/25][3/16] Loss_D: 0.2642 Loss_G: 6.7467\n",
      "0.12465605139732361\n",
      "[41/25][4/16] Loss_D: 0.1247 Loss_G: 6.4967\n",
      "0.3274981677532196\n",
      "[41/25][5/16] Loss_D: 0.3275 Loss_G: 3.2339\n",
      "0.36162373423576355\n",
      "[41/25][6/16] Loss_D: 0.3616 Loss_G: 6.1755\n",
      "0.29484567046165466\n",
      "[41/25][7/16] Loss_D: 0.2948 Loss_G: 5.3210\n",
      "0.1987960934638977\n",
      "[41/25][8/16] Loss_D: 0.1988 Loss_G: 4.6615\n",
      "0.2506476044654846\n",
      "[41/25][9/16] Loss_D: 0.2506 Loss_G: 4.1266\n",
      "0.4032900631427765\n",
      "[41/25][10/16] Loss_D: 0.4033 Loss_G: 6.2839\n",
      "0.1887599527835846\n",
      "[41/25][11/16] Loss_D: 0.1888 Loss_G: 5.3269\n",
      "0.3095172345638275\n",
      "[41/25][12/16] Loss_D: 0.3095 Loss_G: 3.3445\n",
      "0.3178614675998688\n",
      "[41/25][13/16] Loss_D: 0.3179 Loss_G: 4.4711\n",
      "0.18575863540172577\n",
      "[41/25][14/16] Loss_D: 0.1858 Loss_G: 5.3342\n",
      "0.14496006071567535\n",
      "[41/25][15/16] Loss_D: 0.1450 Loss_G: 5.0013\n",
      "0.17164689302444458\n",
      "[42/25][0/16] Loss_D: 0.1716 Loss_G: 3.9828\n",
      "0.1673813760280609\n",
      "[42/25][1/16] Loss_D: 0.1674 Loss_G: 4.5411\n",
      "0.17862428724765778\n",
      "[42/25][2/16] Loss_D: 0.1786 Loss_G: 5.2540\n",
      "0.23023658990859985\n",
      "[42/25][3/16] Loss_D: 0.2302 Loss_G: 4.2911\n",
      "0.2953178584575653\n",
      "[42/25][4/16] Loss_D: 0.2953 Loss_G: 4.1886\n",
      "0.31161701679229736\n",
      "[42/25][5/16] Loss_D: 0.3116 Loss_G: 4.5088\n",
      "0.2844480872154236\n",
      "[42/25][6/16] Loss_D: 0.2844 Loss_G: 4.2196\n",
      "0.20450842380523682\n",
      "[42/25][7/16] Loss_D: 0.2045 Loss_G: 4.7073\n",
      "0.3252880573272705\n",
      "[42/25][8/16] Loss_D: 0.3253 Loss_G: 5.6162\n",
      "0.26984187960624695\n",
      "[42/25][9/16] Loss_D: 0.2698 Loss_G: 4.0903\n",
      "0.4324606657028198\n",
      "[42/25][10/16] Loss_D: 0.4325 Loss_G: 5.6462\n",
      "0.3138863742351532\n",
      "[42/25][11/16] Loss_D: 0.3139 Loss_G: 4.0686\n",
      "0.33938145637512207\n",
      "[42/25][12/16] Loss_D: 0.3394 Loss_G: 4.7697\n",
      "0.21285270154476166\n",
      "[42/25][13/16] Loss_D: 0.2129 Loss_G: 5.0171\n",
      "0.2500423491001129\n",
      "[42/25][14/16] Loss_D: 0.2500 Loss_G: 3.4771\n",
      "0.17924943566322327\n",
      "[42/25][15/16] Loss_D: 0.1792 Loss_G: 4.0807\n",
      "0.3361828923225403\n",
      "[43/25][0/16] Loss_D: 0.3362 Loss_G: 6.9084\n",
      "0.13837800920009613\n",
      "[43/25][1/16] Loss_D: 0.1384 Loss_G: 6.2195\n",
      "0.31908118724823\n",
      "[43/25][2/16] Loss_D: 0.3191 Loss_G: 3.5837\n",
      "0.3927235007286072\n",
      "[43/25][3/16] Loss_D: 0.3927 Loss_G: 6.0489\n",
      "0.15282729268074036\n",
      "[43/25][4/16] Loss_D: 0.1528 Loss_G: 5.6067\n",
      "0.19575555622577667\n",
      "[43/25][5/16] Loss_D: 0.1958 Loss_G: 4.5868\n",
      "0.17735618352890015\n",
      "[43/25][6/16] Loss_D: 0.1774 Loss_G: 4.7008\n",
      "0.24413003027439117\n",
      "[43/25][7/16] Loss_D: 0.2441 Loss_G: 6.5447\n",
      "0.17832010984420776\n",
      "[43/25][8/16] Loss_D: 0.1783 Loss_G: 4.8011\n",
      "0.14131879806518555\n",
      "[43/25][9/16] Loss_D: 0.1413 Loss_G: 4.9164\n",
      "0.1671627163887024\n",
      "[43/25][10/16] Loss_D: 0.1672 Loss_G: 4.6993\n",
      "0.28951355814933777\n",
      "[43/25][11/16] Loss_D: 0.2895 Loss_G: 3.4815\n",
      "0.2774120569229126\n",
      "[43/25][12/16] Loss_D: 0.2774 Loss_G: 6.0422\n",
      "0.15146708488464355\n",
      "[43/25][13/16] Loss_D: 0.1515 Loss_G: 6.0056\n",
      "0.23417149484157562\n",
      "[43/25][14/16] Loss_D: 0.2342 Loss_G: 3.2660\n",
      "0.3629075884819031\n",
      "[43/25][15/16] Loss_D: 0.3629 Loss_G: 7.9493\n",
      "0.24640639126300812\n",
      "[44/25][0/16] Loss_D: 0.2464 Loss_G: 5.4355\n",
      "0.20808887481689453\n",
      "[44/25][1/16] Loss_D: 0.2081 Loss_G: 3.4994\n",
      "0.4065755009651184\n",
      "[44/25][2/16] Loss_D: 0.4066 Loss_G: 7.9740\n",
      "0.3771170675754547\n",
      "[44/25][3/16] Loss_D: 0.3771 Loss_G: 3.6549\n",
      "0.35564619302749634\n",
      "[44/25][4/16] Loss_D: 0.3556 Loss_G: 8.1768\n",
      "0.39505791664123535\n",
      "[44/25][5/16] Loss_D: 0.3951 Loss_G: 5.0096\n",
      "0.3536846935749054\n",
      "[44/25][6/16] Loss_D: 0.3537 Loss_G: 4.9545\n",
      "0.1663796603679657\n",
      "[44/25][7/16] Loss_D: 0.1664 Loss_G: 5.9585\n",
      "0.23747098445892334\n",
      "[44/25][8/16] Loss_D: 0.2375 Loss_G: 3.9030\n",
      "0.4778868556022644\n",
      "[44/25][9/16] Loss_D: 0.4779 Loss_G: 8.1615\n",
      "0.6735281348228455\n",
      "[44/25][10/16] Loss_D: 0.6735 Loss_G: 3.7074\n",
      "0.35701441764831543\n",
      "[44/25][11/16] Loss_D: 0.3570 Loss_G: 5.4364\n",
      "0.09700100868940353\n",
      "[44/25][12/16] Loss_D: 0.0970 Loss_G: 5.8605\n",
      "0.12424711883068085\n",
      "[44/25][13/16] Loss_D: 0.1242 Loss_G: 4.9892\n",
      "0.2036246657371521\n",
      "[44/25][14/16] Loss_D: 0.2036 Loss_G: 5.5025\n",
      "0.37044429779052734\n",
      "[44/25][15/16] Loss_D: 0.3704 Loss_G: 2.3381\n",
      "1.1622605323791504\n",
      "[45/25][0/16] Loss_D: 1.1623 Loss_G: 11.5045\n",
      "2.3394978046417236\n",
      "[45/25][1/16] Loss_D: 2.3395 Loss_G: 2.2472\n",
      "0.7272428274154663\n",
      "[45/25][2/16] Loss_D: 0.7272 Loss_G: 7.5205\n",
      "0.40634283423423767\n",
      "[45/25][3/16] Loss_D: 0.4063 Loss_G: 4.8029\n",
      "0.2545655369758606\n",
      "[45/25][4/16] Loss_D: 0.2546 Loss_G: 4.2962\n",
      "0.5795086026191711\n",
      "[45/25][5/16] Loss_D: 0.5795 Loss_G: 6.2555\n",
      "0.3156229257583618\n",
      "[45/25][6/16] Loss_D: 0.3156 Loss_G: 4.7884\n",
      "0.2700974941253662\n",
      "[45/25][7/16] Loss_D: 0.2701 Loss_G: 5.5852\n",
      "0.23106756806373596\n",
      "[45/25][8/16] Loss_D: 0.2311 Loss_G: 6.4510\n",
      "0.45746999979019165\n",
      "[45/25][9/16] Loss_D: 0.4575 Loss_G: 3.8693\n",
      "0.8004613518714905\n",
      "[45/25][10/16] Loss_D: 0.8005 Loss_G: 9.3466\n",
      "1.1286718845367432\n",
      "[45/25][11/16] Loss_D: 1.1287 Loss_G: 4.9708\n",
      "0.1142902821302414\n",
      "[45/25][12/16] Loss_D: 0.1143 Loss_G: 3.6001\n",
      "0.36431440711021423\n",
      "[45/25][13/16] Loss_D: 0.3643 Loss_G: 6.7326\n",
      "0.06254132091999054\n",
      "[45/25][14/16] Loss_D: 0.0625 Loss_G: 6.6594\n",
      "0.14375673234462738\n",
      "[45/25][15/16] Loss_D: 0.1438 Loss_G: 5.2355\n",
      "0.11370480060577393\n",
      "[46/25][0/16] Loss_D: 0.1137 Loss_G: 3.9832\n",
      "0.1671844720840454\n",
      "[46/25][1/16] Loss_D: 0.1672 Loss_G: 4.5039\n",
      "0.17764651775360107\n",
      "[46/25][2/16] Loss_D: 0.1776 Loss_G: 4.5952\n",
      "0.17479699850082397\n",
      "[46/25][3/16] Loss_D: 0.1748 Loss_G: 4.5044\n",
      "0.1979728639125824\n",
      "[46/25][4/16] Loss_D: 0.1980 Loss_G: 4.6221\n",
      "0.1444382518529892\n",
      "[46/25][5/16] Loss_D: 0.1444 Loss_G: 5.2470\n",
      "0.2419973611831665\n",
      "[46/25][6/16] Loss_D: 0.2420 Loss_G: 4.1770\n",
      "0.2906136214733124\n",
      "[46/25][7/16] Loss_D: 0.2906 Loss_G: 5.4913\n",
      "0.24603357911109924\n",
      "[46/25][8/16] Loss_D: 0.2460 Loss_G: 6.0454\n",
      "0.5309139490127563\n",
      "[46/25][9/16] Loss_D: 0.5309 Loss_G: 5.2449\n",
      "0.44241803884506226\n",
      "[46/25][10/16] Loss_D: 0.4424 Loss_G: 5.0768\n",
      "0.451738178730011\n",
      "[46/25][11/16] Loss_D: 0.4517 Loss_G: 5.9843\n",
      "0.310757577419281\n",
      "[46/25][12/16] Loss_D: 0.3108 Loss_G: 5.8881\n",
      "0.32844096422195435\n",
      "[46/25][13/16] Loss_D: 0.3284 Loss_G: 5.6440\n",
      "0.3106842041015625\n",
      "[46/25][14/16] Loss_D: 0.3107 Loss_G: 5.7961\n",
      "0.8247758150100708\n",
      "[46/25][15/16] Loss_D: 0.8248 Loss_G: 6.3826\n",
      "0.512752115726471\n",
      "[47/25][0/16] Loss_D: 0.5128 Loss_G: 4.2911\n",
      "0.7394335269927979\n",
      "[47/25][1/16] Loss_D: 0.7394 Loss_G: 11.0278\n",
      "1.7499216794967651\n",
      "[47/25][2/16] Loss_D: 1.7499 Loss_G: 4.8636\n",
      "0.36035892367362976\n",
      "[47/25][3/16] Loss_D: 0.3604 Loss_G: 5.2338\n",
      "0.23867538571357727\n",
      "[47/25][4/16] Loss_D: 0.2387 Loss_G: 6.5837\n",
      "0.25894126296043396\n",
      "[47/25][5/16] Loss_D: 0.2589 Loss_G: 6.6240\n",
      "0.24789930880069733\n",
      "[47/25][6/16] Loss_D: 0.2479 Loss_G: 5.4958\n",
      "0.4115790128707886\n",
      "[47/25][7/16] Loss_D: 0.4116 Loss_G: 6.8761\n",
      "0.30998140573501587\n",
      "[47/25][8/16] Loss_D: 0.3100 Loss_G: 4.8278\n",
      "0.3156259059906006\n",
      "[47/25][9/16] Loss_D: 0.3156 Loss_G: 4.7775\n",
      "0.226392462849617\n",
      "[47/25][10/16] Loss_D: 0.2264 Loss_G: 6.6253\n",
      "0.16303622722625732\n",
      "[47/25][11/16] Loss_D: 0.1630 Loss_G: 6.3688\n",
      "0.18742167949676514\n",
      "[47/25][12/16] Loss_D: 0.1874 Loss_G: 5.4577\n",
      "0.3338329792022705\n",
      "[47/25][13/16] Loss_D: 0.3338 Loss_G: 6.4914\n",
      "0.5580224990844727\n",
      "[47/25][14/16] Loss_D: 0.5580 Loss_G: 3.5778\n",
      "0.5930712223052979\n",
      "[47/25][15/16] Loss_D: 0.5931 Loss_G: 10.4572\n",
      "0.7590259909629822\n",
      "[48/25][0/16] Loss_D: 0.7590 Loss_G: 4.4175\n",
      "0.39690646529197693\n",
      "[48/25][1/16] Loss_D: 0.3969 Loss_G: 7.0281\n",
      "0.3041127920150757\n",
      "[48/25][2/16] Loss_D: 0.3041 Loss_G: 5.0518\n",
      "0.2850685715675354\n",
      "[48/25][3/16] Loss_D: 0.2851 Loss_G: 6.0647\n",
      "0.40933212637901306\n",
      "[48/25][4/16] Loss_D: 0.4093 Loss_G: 3.8067\n",
      "0.3770392835140228\n",
      "[48/25][5/16] Loss_D: 0.3770 Loss_G: 5.5040\n",
      "0.3584982454776764\n",
      "[48/25][6/16] Loss_D: 0.3585 Loss_G: 4.9294\n",
      "0.32651689648628235\n",
      "[48/25][7/16] Loss_D: 0.3265 Loss_G: 3.5749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35357218980789185\n",
      "[48/25][8/16] Loss_D: 0.3536 Loss_G: 4.5470\n",
      "0.20158708095550537\n",
      "[48/25][9/16] Loss_D: 0.2016 Loss_G: 4.4275\n",
      "0.21645136177539825\n",
      "[48/25][10/16] Loss_D: 0.2165 Loss_G: 3.9033\n",
      "0.1641497015953064\n",
      "[48/25][11/16] Loss_D: 0.1641 Loss_G: 5.3301\n",
      "0.16688068211078644\n",
      "[48/25][12/16] Loss_D: 0.1669 Loss_G: 4.5988\n",
      "0.11321272701025009\n",
      "[48/25][13/16] Loss_D: 0.1132 Loss_G: 4.7040\n",
      "0.16213800013065338\n",
      "[48/25][14/16] Loss_D: 0.1621 Loss_G: 5.1683\n",
      "0.1033392921090126\n",
      "[48/25][15/16] Loss_D: 0.1033 Loss_G: 5.3776\n",
      "0.10740303993225098\n",
      "[49/25][0/16] Loss_D: 0.1074 Loss_G: 4.8964\n",
      "0.26843950152397156\n",
      "[49/25][1/16] Loss_D: 0.2684 Loss_G: 3.0956\n",
      "0.5665212273597717\n",
      "[49/25][2/16] Loss_D: 0.5665 Loss_G: 8.9753\n",
      "1.1511589288711548\n",
      "[49/25][3/16] Loss_D: 1.1512 Loss_G: 4.0330\n",
      "0.25007742643356323\n",
      "[49/25][4/16] Loss_D: 0.2501 Loss_G: 4.0071\n",
      "0.2486061006784439\n",
      "[49/25][5/16] Loss_D: 0.2486 Loss_G: 6.3331\n",
      "0.08079415559768677\n",
      "[49/25][6/16] Loss_D: 0.0808 Loss_G: 6.4479\n",
      "0.15789513289928436\n",
      "[49/25][7/16] Loss_D: 0.1579 Loss_G: 4.9072\n",
      "0.20853400230407715\n",
      "[49/25][8/16] Loss_D: 0.2085 Loss_G: 4.7593\n",
      "0.5381177663803101\n",
      "[49/25][9/16] Loss_D: 0.5381 Loss_G: 6.4501\n",
      "0.25484612584114075\n",
      "[49/25][10/16] Loss_D: 0.2548 Loss_G: 4.2796\n",
      "0.3016650080680847\n",
      "[49/25][11/16] Loss_D: 0.3017 Loss_G: 4.2039\n",
      "0.2562093436717987\n",
      "[49/25][12/16] Loss_D: 0.2562 Loss_G: 6.4186\n",
      "0.264141321182251\n",
      "[49/25][13/16] Loss_D: 0.2641 Loss_G: 4.8278\n",
      "0.26352596282958984\n",
      "[49/25][14/16] Loss_D: 0.2635 Loss_G: 5.7095\n",
      "0.3625345230102539\n",
      "[49/25][15/16] Loss_D: 0.3625 Loss_G: 4.8921\n"
     ]
    }
   ],
   "source": [
    "# Deep Convolutional GANs\n",
    "\n",
    "# Importing the libraries\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "\n",
    "# Setting some hyperparameters\n",
    "batchSize = 64 # We set the size of the batch.\n",
    "imageSize = 64 # We set the size of the generated images (64x64).\n",
    "\n",
    "# Creating the transformations\n",
    "transform = transforms.Compose([transforms.Resize(256),transforms.Scale(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) # We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.\n",
    "\n",
    "# Loading the dataset\n",
    "#dataset = dset.CIFAR10(root = './data', download = True, transform = transform) # We download the training set in the ./data folder and we apply the previous transformations on each image.\n",
    "#dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2) # We use dataLoader to get the images of the training set batch by batch.\n",
    "#dataset = dset.ImageFolder(root=\"./images/\", transform=transform)\n",
    "#dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2)\n",
    "dataset = dset.ImageFolder(root=\"./images\",\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(imageSize),\n",
    "                               transforms.CenterCrop(imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "print(\"AAAAAA\")\n",
    "\n",
    "\n",
    "# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "# Defining the generator\n",
    "\n",
    "\n",
    "class G(nn.Module): # We introduce a class to define the generator.\n",
    "\n",
    "    def __init__(self): # We introduce the __init__() function that will define the architecture of the generator.\n",
    "        super(G, self).__init__() # We inherit from the nn.Module tools and activate inheritance\n",
    "        self.main = nn.Sequential( # We create a meta module of a neural network that will contain a sequence of modules (convolutions, full connections, etc.).\n",
    "            # inverse conv take input as random vector and return image, 100- size of input vector, 512- number of feature map of o/p, 4- size of kernel, 1- stride, 0-padding\n",
    "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False), # We start with an inversed convolution. inverse cnn\n",
    "            nn.BatchNorm2d(512), # We normalize all the features along the dimension of the batch.\n",
    "            nn.ReLU(True), # We apply a ReLU rectification to break the linearity.\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False), # We add another inversed convolution.\n",
    "            nn.BatchNorm2d(256), # We normalize again.\n",
    "            nn.ReLU(True), # We apply another ReLU.\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False), # We add another inversed convolution.\n",
    "            nn.BatchNorm2d(128), # We normalize again.\n",
    "            nn.ReLU(True), # We apply another ReLU.\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False), # We add another inversed convolution.\n",
    "            nn.BatchNorm2d(64), # We normalize again.\n",
    "            nn.ReLU(True), # We apply another ReLU.\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False), # We add another inversed convolution.\n",
    "            nn.Tanh() # We apply a Tanh rectification to break the linearity and stay between -1 and +1.\n",
    "        )\n",
    "\n",
    "    def forward(self, input): # We define the forward function that takes as argument an input that will be fed to the neural network, and that will return the output containing the generated images.\n",
    "        output = self.main(input) # We forward propagate the signal through the whole neural network of the generator defined by self.main.\n",
    "        return output # We return the output containing the generated images.\n",
    "\n",
    "\n",
    "# Creating the generator\n",
    "netG = G() # We create the generator object.\n",
    "netG.apply(weights_init) # We initialize all the weights of its neural network.\n",
    "\n",
    "# Defining the discriminator\n",
    "\n",
    "class D(nn.Module): # We introduce a class to define the discriminator.\n",
    "\n",
    "    def __init__(self): # We introduce the __init__() function that will define the architecture of the discriminator.\n",
    "        super(D, self).__init__() # We inherit from the nn.Module tools.\n",
    "        self.main = nn.Sequential( # We create a meta module of a neural network that will contain a sequence of modules (convolutions, full connections, etc.).\n",
    "            # 3- channel of generated image generated by generated\n",
    "            # 64-  feature map, 4- kernel size, 2- stride, 1- padding\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias = False), # We start with a convolution.\n",
    "            nn.LeakyReLU(0.2, inplace = True), # We apply a LeakyReLU.\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias = False), # We add another convolution.\n",
    "            nn.BatchNorm2d(128), # We normalize all the features along the dimension of the batch.\n",
    "            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias = False), # We add another convolution.\n",
    "            nn.BatchNorm2d(256), # We normalize again.\n",
    "            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias = False), # We add another convolution.\n",
    "            nn.BatchNorm2d(512), # We normalize again.\n",
    "            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias = False), # We add another convolution.\n",
    "            # why sigmoid\n",
    "            nn.Sigmoid() # We apply a Sigmoid rectification to break the linearity and stay between 0 and 1.\n",
    "        )\n",
    "\n",
    "    def forward(self, input): # We define the forward function that takes as argument an input that will be fed to the neural network, and that will return the output which will be a value between 0 and 1.\n",
    "        output = self.main(input) # We forward propagate the signal through the whole neural network of the discriminator defined by self.main.\n",
    "        return output.view(-1) # We return the output which will be a value between 0 and 1., view(-1) flaten the the results from the convolution layer\n",
    "\n",
    "# Creating the discriminator\n",
    "netD = D() # We create the discriminator object.\n",
    "netD.apply(weights_init) # We initialize all the weights of its neural network.\n",
    "\n",
    "# Training the DCGANs\n",
    "\n",
    "criterion = nn.BCELoss() # We create a criterion object that will measure the error between the prediction and the target.\n",
    "# lr- learning rate\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # We create the optimizer object of the discriminator.\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # We create the optimizer object of the generator.\n",
    "\n",
    "for epoch in range(50): # We iterate over 25 epochs.\n",
    "    # i = index, data- mini batch of the images\n",
    "    for i, data in enumerate(dataloader, 0): # We iterate over the images of the dataset.\n",
    "        \n",
    "        # 1st Step: Updating the weights of the neural network of the discriminator\n",
    "\n",
    "        netD.zero_grad() # We initialize to 0 the gradients of the discriminator with respect to the weights.\n",
    "        \n",
    "        # Training the discriminator with a real image of the dataset\n",
    "        real, _ = data # We get a real image of the dataset which will be used to train the discriminator., 1st element\n",
    "        input = Variable(real) # We wrap it in a variable. torch variable, minibach or images in a torch variable\n",
    "        target = Variable(torch.ones(input.size()[0])) # We get the target.input.size()[0] is the size of the mini batch\n",
    "        output = netD(input) # We forward propagate this real image into the neural network of the discriminator to get the prediction (a value between 0 and 1).\n",
    "        errD_real = criterion(output, target) # We compute the loss between the predictions (output) and the target (equal to 1).\n",
    "        \n",
    "        # Training the discriminator with a fake image generated by the generator, size of vector size 100, as seen in 1st conv layer of generator\n",
    "        noise = Variable(torch.randn(input.size()[0], 100, 1, 1)) # We make a random input vector (noise) of the generator.\n",
    "        fake = netG(noise) # We forward propagate this random input vector into the neural network of the generator to get some fake generated images.\n",
    "        target = Variable(torch.zeros(input.size()[0])) # We get the target.\n",
    "        # detach means in this step we do not require gradient so we detach it\n",
    "        # detach means in this step we do not require gradient so we detach it\n",
    "        output = netD(fake.detach()) # We forward propagate the fake generated images into the neural network of the discriminator to get the prediction (a value between 0 and 1).\n",
    "        errD_fake = criterion(output, target) # We compute the loss between the prediction (output) and the target (equal to 0).\n",
    "\n",
    "        # Backpropagating the total error\n",
    "        errD = errD_real + errD_fake # We compute the total error of the discriminator.\n",
    "        errD.backward() # We backpropagate the loss error by computing the gradients of the total error with respect to the weights of the discriminator.\n",
    "        optimizerD.step() # We apply the optimizer to update the weights according to how much they are responsible for the loss error of the discriminator.\n",
    "\n",
    "        # 2nd Step: Updating the weights of the neural network of the generator\n",
    "\n",
    "        netG.zero_grad() # We initialize to 0 the gradients of the generator with respect to the weights.\n",
    "        target = Variable(torch.ones(input.size()[0])) # We get the target.\n",
    "        output = netD(fake) # We forward propagate the fake generated images into the neural network of the discriminator to get the prediction (a value between 0 and 1).\n",
    "        errG = criterion(output, target) # We compute the loss between the prediction (output between 0 and 1) and the target (equal to 1).\n",
    "        errG.backward() # We backpropagate the loss error by computing the gradients of the total error with respect to the weights of the generator.\n",
    "        optimizerG.step() # We apply the optimizer to update the weights according to how much they are responsible for the loss error of the generator.\n",
    "        \n",
    "        # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps\n",
    "        print(errD.data.item())\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 25, i, len(dataloader), errD.data.item(), errG.data.item())) # We print les losses of the discriminator (Loss_D) and the generator (Loss_G).\n",
    "        if i % 100 == 0: # Every 100 steps:\n",
    "            vutils.save_image(real, '%s/real_samples.png' % \"./results\", normalize = True) # We save the real images of the minibatch.\n",
    "            fake = netG(noise) # We get our fake generated images.\n",
    "            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"./results\", epoch), normalize = True) # We also save the fake generated images of the minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
